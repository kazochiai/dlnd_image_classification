{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:37, 4.52MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/cifar/cifar-10-python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f83f38c8f28>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.94509804  0.78431373  0.42745098]\n",
      "   [ 0.6         0.17254902  0.94117647]\n",
      "   [ 0.74117647  0.14901961  0.1372549 ]\n",
      "   ..., \n",
      "   [ 0.01568627  0.77647059  0.1372549 ]\n",
      "   [ 0.38431373  0.88235294  0.90588235]\n",
      "   [ 0.17647059  0.55294118  0.85882353]]\n",
      "\n",
      "  [[ 0.7254902   0.76470588  0.9254902 ]\n",
      "   [ 0.78431373  0.2627451   0.14901961]\n",
      "   [ 0.87843137  0.2627451   0.2745098 ]\n",
      "   ..., \n",
      "   [ 0.44705882  0.42352941  0.48235294]\n",
      "   [ 0.77647059  0.22352941  0.56470588]\n",
      "   [ 0.70980392  0.7254902   0.2       ]]\n",
      "\n",
      "  [[ 0.3254902   0.91372549  0.60784314]\n",
      "   [ 0.90588235  0.85490196  0.19607843]\n",
      "   [ 0.70196078  0.42352941  0.94117647]\n",
      "   ..., \n",
      "   [ 0.95294118  0.44313725  0.97647059]\n",
      "   [ 0.45098039  0.91372549  0.24313725]\n",
      "   [ 0.69803922  0.30588235  0.55294118]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.11372549  0.2627451   0.55686275]\n",
      "   [ 0.09803922  0.44313725  0.07843137]\n",
      "   [ 0.7372549   0.00392157  0.76078431]\n",
      "   ..., \n",
      "   [ 0.52156863  0.05882353  0.85490196]\n",
      "   [ 0.56078431  0.75686275  0.47843137]\n",
      "   [ 0.24313725  0.76470588  0.6745098 ]]\n",
      "\n",
      "  [[ 0.70196078  0.78431373  0.96078431]\n",
      "   [ 0.77254902  0.60392157  0.80392157]\n",
      "   [ 0.68627451  0.22352941  0.34901961]\n",
      "   ..., \n",
      "   [ 0.18431373  0.97254902  0.54509804]\n",
      "   [ 0.40784314  0.10196078  0.62745098]\n",
      "   [ 0.6         0.76470588  0.40784314]]\n",
      "\n",
      "  [[ 0.55686275  0.65098039  0.70196078]\n",
      "   [ 0.30588235  0.62352941  0.09411765]\n",
      "   [ 0.60392157  0.40784314  0.06666667]\n",
      "   ..., \n",
      "   [ 0.42745098  0.28627451  0.70980392]\n",
      "   [ 0.33333333  0.06666667  0.80392157]\n",
      "   [ 0.87843137  0.81568627  0.94117647]]]\n",
      "\n",
      "\n",
      " [[[ 0.74901961  0.8745098   0.90980392]\n",
      "   [ 0.38823529  0.32156863  0.32941176]\n",
      "   [ 0.41960784  0.5254902   0.2745098 ]\n",
      "   ..., \n",
      "   [ 0.34901961  0.48627451  0.21568627]\n",
      "   [ 0.69019608  0.74901961  0.63529412]\n",
      "   [ 0.03137255  0.88235294  0.86666667]]\n",
      "\n",
      "  [[ 0.35686275  0.4627451   0.67843137]\n",
      "   [ 0.83529412  0.42352941  0.6745098 ]\n",
      "   [ 0.37254902  0.3254902   0.23921569]\n",
      "   ..., \n",
      "   [ 0.45490196  0.99607843  0.20392157]\n",
      "   [ 0.83921569  0.54509804  0.48627451]\n",
      "   [ 0.80784314  0.12941176  0.55294118]]\n",
      "\n",
      "  [[ 0.88235294  0.61568627  0.8627451 ]\n",
      "   [ 0.68235294  0.94117647  0.28235294]\n",
      "   [ 0.29803922  0.8745098   0.13333333]\n",
      "   ..., \n",
      "   [ 0.28627451  0.16078431  0.80784314]\n",
      "   [ 0.82745098  0.28235294  0.34509804]\n",
      "   [ 0.94901961  0.23137255  1.        ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.81960784  0.08627451  0.74117647]\n",
      "   [ 0.54117647  0.7372549   0.        ]\n",
      "   [ 0.3372549   0.3254902   0.26666667]\n",
      "   ..., \n",
      "   [ 0.32941176  0.79607843  0.80784314]\n",
      "   [ 0.01568627  0.36862745  0.09411765]\n",
      "   [ 0.34509804  0.03529412  0.29019608]]\n",
      "\n",
      "  [[ 0.21960784  0.2         0.05882353]\n",
      "   [ 0.05882353  0.17647059  0.89019608]\n",
      "   [ 0.21568627  0.22745098  0.28235294]\n",
      "   ..., \n",
      "   [ 0.6         0.56078431  0.76862745]\n",
      "   [ 0.18039216  0.34117647  0.55686275]\n",
      "   [ 0.5254902   0.6745098   0.86666667]]\n",
      "\n",
      "  [[ 0.08235294  0.4745098   0.82352941]\n",
      "   [ 0.87058824  0.63137255  0.8745098 ]\n",
      "   [ 0.01960784  0.58039216  0.22745098]\n",
      "   ..., \n",
      "   [ 0.6627451   0.70588235  0.97254902]\n",
      "   [ 0.64313725  0.98039216  0.7372549 ]\n",
      "   [ 0.38431373  0.05098039  0.9254902 ]]]\n",
      "\n",
      "\n",
      " [[[ 0.41960784  0.16078431  0.8745098 ]\n",
      "   [ 0.61568627  0.68627451  0.57254902]\n",
      "   [ 0.4627451   0.16862745  0.14117647]\n",
      "   ..., \n",
      "   [ 0.54509804  0.81568627  0.80784314]\n",
      "   [ 0.84313725  0.86666667  0.09803922]\n",
      "   [ 0.34117647  0.89803922  0.09411765]]\n",
      "\n",
      "  [[ 0.39215686  0.17254902  0.72941176]\n",
      "   [ 0.42352941  0.51764706  0.70588235]\n",
      "   [ 0.12941176  0.24705882  0.84313725]\n",
      "   ..., \n",
      "   [ 0.07058824  0.71372549  1.        ]\n",
      "   [ 0.7372549   0.33333333  0.85098039]\n",
      "   [ 0.41176471  0.2745098   0.93333333]]\n",
      "\n",
      "  [[ 0.31372549  0.87058824  0.69803922]\n",
      "   [ 0.9254902   0.23921569  0.33333333]\n",
      "   [ 0.36470588  0.11764706  0.75686275]\n",
      "   ..., \n",
      "   [ 0.90980392  0.7254902   0.57254902]\n",
      "   [ 0.00784314  0.34117647  0.16862745]\n",
      "   [ 0.57254902  0.05490196  0.58039216]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.9254902   0.18431373  0.60784314]\n",
      "   [ 0.55294118  0.52941176  0.09019608]\n",
      "   [ 0.01176471  0.30588235  0.33333333]\n",
      "   ..., \n",
      "   [ 0.30588235  0.15294118  0.10588235]\n",
      "   [ 0.34901961  0.49411765  0.56470588]\n",
      "   [ 0.46666667  0.37254902  0.4       ]]\n",
      "\n",
      "  [[ 0.70980392  0.42745098  0.0745098 ]\n",
      "   [ 0.55294118  0.11764706  0.76470588]\n",
      "   [ 0.63137255  0.80784314  0.56862745]\n",
      "   ..., \n",
      "   [ 0.74117647  0.01568627  0.9254902 ]\n",
      "   [ 0.58039216  0.98431373  0.26666667]\n",
      "   [ 0.96470588  0.06666667  0.68627451]]\n",
      "\n",
      "  [[ 0.35686275  0.03529412  0.96078431]\n",
      "   [ 0.66666667  0.91764706  0.32156863]\n",
      "   [ 0.52156863  0.46666667  0.4627451 ]\n",
      "   ..., \n",
      "   [ 0.78039216  0.23137255  0.00392157]\n",
      "   [ 0.98823529  0.64313725  0.56078431]\n",
      "   [ 0.97254902  0.58823529  0.2745098 ]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.82745098  0.98039216  0.96470588]\n",
      "   [ 0.49803922  0.35294118  0.37647059]\n",
      "   [ 0.22352941  0.10588235  0.22352941]\n",
      "   ..., \n",
      "   [ 1.          0.65098039  0.14117647]\n",
      "   [ 0.1254902   0.40784314  0.34509804]\n",
      "   [ 0.55686275  0.05098039  0.95686275]]\n",
      "\n",
      "  [[ 0.98431373  0.65490196  0.65882353]\n",
      "   [ 0.85882353  0.61176471  0.9372549 ]\n",
      "   [ 0.5254902   0.47058824  0.88627451]\n",
      "   ..., \n",
      "   [ 0.30196078  0.75686275  0.32941176]\n",
      "   [ 0.65882353  0.84313725  0.85098039]\n",
      "   [ 0.85882353  0.89411765  0.09803922]]\n",
      "\n",
      "  [[ 0.01568627  0.75686275  0.79607843]\n",
      "   [ 0.92156863  0.79607843  0.34117647]\n",
      "   [ 0.78039216  0.49019608  0.97254902]\n",
      "   ..., \n",
      "   [ 0.67058824  0.69019608  0.61568627]\n",
      "   [ 0.63529412  0.81568627  0.50588235]\n",
      "   [ 0.39215686  0.76862745  0.69803922]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.25098039  0.70588235  0.50196078]\n",
      "   [ 0.77647059  0.76470588  0.63137255]\n",
      "   [ 0.57254902  0.14901961  0.91372549]\n",
      "   ..., \n",
      "   [ 0.4745098   0.26666667  0.69803922]\n",
      "   [ 0.15686275  0.81960784  0.7372549 ]\n",
      "   [ 0.02745098  0.59215686  0.19607843]]\n",
      "\n",
      "  [[ 0.04313725  0.0627451   0.79607843]\n",
      "   [ 0.99215686  0.31764706  0.3254902 ]\n",
      "   [ 0.16078431  0.7254902   0.84313725]\n",
      "   ..., \n",
      "   [ 0.36862745  0.90588235  0.74509804]\n",
      "   [ 0.78039216  0.41960784  0.06666667]\n",
      "   [ 0.27058824  0.84705882  0.05098039]]\n",
      "\n",
      "  [[ 0.78431373  0.14117647  0.97254902]\n",
      "   [ 0.61568627  0.00392157  0.08627451]\n",
      "   [ 0.81960784  0.8745098   0.94509804]\n",
      "   ..., \n",
      "   [ 0.36862745  0.24705882  0.88235294]\n",
      "   [ 0.38039216  0.25098039  0.23137255]\n",
      "   [ 0.90588235  0.65882353  0.64705882]]]\n",
      "\n",
      "\n",
      " [[[ 0.72941176  0.95294118  0.58431373]\n",
      "   [ 0.42352941  0.82745098  0.90980392]\n",
      "   [ 0.96862745  0.01960784  0.87058824]\n",
      "   ..., \n",
      "   [ 0.49019608  0.34901961  0.39607843]\n",
      "   [ 0.56470588  0.14509804  0.31372549]\n",
      "   [ 0.32156863  0.80784314  0.55686275]]\n",
      "\n",
      "  [[ 0.99215686  0.07058824  0.56078431]\n",
      "   [ 0.63529412  0.89803922  0.23529412]\n",
      "   [ 0.41960784  0.73333333  0.85490196]\n",
      "   ..., \n",
      "   [ 0.          0.51764706  0.44313725]\n",
      "   [ 0.63529412  0.17647059  0.57254902]\n",
      "   [ 0.06666667  0.50980392  0.52156863]]\n",
      "\n",
      "  [[ 0.74901961  0.84705882  0.46666667]\n",
      "   [ 0.87843137  0.93333333  0.86666667]\n",
      "   [ 0.53333333  0.94901961  0.95294118]\n",
      "   ..., \n",
      "   [ 0.84705882  0.45490196  0.6       ]\n",
      "   [ 0.50980392  0.46666667  0.48627451]\n",
      "   [ 0.09803922  0.61568627  0.74117647]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.70588235  0.94509804  0.70588235]\n",
      "   [ 0.10196078  0.57254902  0.36862745]\n",
      "   [ 0.70196078  0.95686275  0.47843137]\n",
      "   ..., \n",
      "   [ 0.75294118  0.49803922  0.02745098]\n",
      "   [ 0.2745098   0.94509804  0.10196078]\n",
      "   [ 0.69019608  0.60784314  0.68627451]]\n",
      "\n",
      "  [[ 0.35294118  0.28235294  0.43137255]\n",
      "   [ 0.40392157  0.70196078  0.56078431]\n",
      "   [ 0.08627451  0.68627451  0.59215686]\n",
      "   ..., \n",
      "   [ 0.84313725  0.43921569  0.82352941]\n",
      "   [ 0.81960784  0.40784314  0.43137255]\n",
      "   [ 0.52941176  0.94901961  0.69019608]]\n",
      "\n",
      "  [[ 0.07058824  0.94901961  0.59215686]\n",
      "   [ 0.48627451  0.62745098  0.34117647]\n",
      "   [ 0.54901961  0.44313725  0.3254902 ]\n",
      "   ..., \n",
      "   [ 0.79607843  0.17254902  0.49411765]\n",
      "   [ 0.92156863  0.95686275  0.92941176]\n",
      "   [ 0.09803922  0.62352941  0.70980392]]]\n",
      "\n",
      "\n",
      " [[[ 0.83529412  0.31764706  0.01568627]\n",
      "   [ 0.05098039  0.84705882  0.99215686]\n",
      "   [ 0.84705882  0.88627451  0.09019608]\n",
      "   ..., \n",
      "   [ 0.98823529  0.48235294  0.75294118]\n",
      "   [ 0.91372549  0.88235294  0.18039216]\n",
      "   [ 0.41176471  0.46666667  0.91764706]]\n",
      "\n",
      "  [[ 0.25098039  0.73333333  0.15294118]\n",
      "   [ 0.69411765  0.19607843  0.92941176]\n",
      "   [ 0.65490196  0.34901961  0.65490196]\n",
      "   ..., \n",
      "   [ 0.71372549  0.34901961  0.08627451]\n",
      "   [ 0.05490196  0.13333333  0.92156863]\n",
      "   [ 0.83137255  0.67058824  0.01176471]]\n",
      "\n",
      "  [[ 0.74901961  0.45490196  0.18823529]\n",
      "   [ 0.44313725  0.80784314  0.56862745]\n",
      "   [ 0.52941176  0.02352941  0.00392157]\n",
      "   ..., \n",
      "   [ 0.56078431  0.19215686  0.61960784]\n",
      "   [ 0.61960784  0.34117647  0.12156863]\n",
      "   [ 0.82745098  0.24705882  0.69019608]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.64705882  0.96078431  0.50196078]\n",
      "   [ 0.96470588  0.2745098   0.52941176]\n",
      "   [ 0.77647059  0.51764706  0.64705882]\n",
      "   ..., \n",
      "   [ 0.69803922  0.67843137  0.30980392]\n",
      "   [ 0.68235294  0.83529412  0.93333333]\n",
      "   [ 0.43529412  0.43921569  0.77254902]]\n",
      "\n",
      "  [[ 0.10588235  0.02352941  0.99215686]\n",
      "   [ 0.43137255  0.38039216  0.46666667]\n",
      "   [ 0.41568627  0.90980392  0.29411765]\n",
      "   ..., \n",
      "   [ 0.49411765  0.38823529  0.17647059]\n",
      "   [ 0.6627451   0.25490196  0.2       ]\n",
      "   [ 0.52941176  0.07843137  0.65490196]]\n",
      "\n",
      "  [[ 0.42352941  0.63529412  0.10196078]\n",
      "   [ 0.04313725  0.09803922  0.59215686]\n",
      "   [ 0.18823529  0.78823529  0.45490196]\n",
      "   ..., \n",
      "   [ 0.81176471  0.4745098   0.4       ]\n",
      "   [ 0.5372549   0.00392157  0.63137255]\n",
      "   [ 0.13333333  0.41568627  0.45098039]]]]\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a = 0.0\n",
    "    b = 1.0\n",
    "    mi = 0.0\n",
    "    ma = 255.0\n",
    "    print(a + ( ( (x - mi)*(b - a) )/( ma - mi ) ))\n",
    "    return a + ( ( (x - mi)*(b - a) )/( ma - mi ) )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "once = False\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    global once\n",
    "    # TODO: Implement Function\n",
    "    if (once == False):\n",
    "        encoder.fit(x)\n",
    "        once = True\n",
    "        \n",
    "    one_hor_labels = encoder.transform(x)\n",
    "\n",
    "    return one_hor_labels\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.23137255  0.24313725  0.24705882]\n",
      "   [ 0.16862745  0.18039216  0.17647059]\n",
      "   [ 0.19607843  0.18823529  0.16862745]\n",
      "   ..., \n",
      "   [ 0.61960784  0.51764706  0.42352941]\n",
      "   [ 0.59607843  0.49019608  0.4       ]\n",
      "   [ 0.58039216  0.48627451  0.40392157]]\n",
      "\n",
      "  [[ 0.0627451   0.07843137  0.07843137]\n",
      "   [ 0.          0.          0.        ]\n",
      "   [ 0.07058824  0.03137255  0.        ]\n",
      "   ..., \n",
      "   [ 0.48235294  0.34509804  0.21568627]\n",
      "   [ 0.46666667  0.3254902   0.19607843]\n",
      "   [ 0.47843137  0.34117647  0.22352941]]\n",
      "\n",
      "  [[ 0.09803922  0.09411765  0.08235294]\n",
      "   [ 0.0627451   0.02745098  0.        ]\n",
      "   [ 0.19215686  0.10588235  0.03137255]\n",
      "   ..., \n",
      "   [ 0.4627451   0.32941176  0.19607843]\n",
      "   [ 0.47058824  0.32941176  0.19607843]\n",
      "   [ 0.42745098  0.28627451  0.16470588]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.81568627  0.66666667  0.37647059]\n",
      "   [ 0.78823529  0.6         0.13333333]\n",
      "   [ 0.77647059  0.63137255  0.10196078]\n",
      "   ..., \n",
      "   [ 0.62745098  0.52156863  0.2745098 ]\n",
      "   [ 0.21960784  0.12156863  0.02745098]\n",
      "   [ 0.20784314  0.13333333  0.07843137]]\n",
      "\n",
      "  [[ 0.70588235  0.54509804  0.37647059]\n",
      "   [ 0.67843137  0.48235294  0.16470588]\n",
      "   [ 0.72941176  0.56470588  0.11764706]\n",
      "   ..., \n",
      "   [ 0.72156863  0.58039216  0.36862745]\n",
      "   [ 0.38039216  0.24313725  0.13333333]\n",
      "   [ 0.3254902   0.20784314  0.13333333]]\n",
      "\n",
      "  [[ 0.69411765  0.56470588  0.45490196]\n",
      "   [ 0.65882353  0.50588235  0.36862745]\n",
      "   [ 0.70196078  0.55686275  0.34117647]\n",
      "   ..., \n",
      "   [ 0.84705882  0.72156863  0.54901961]\n",
      "   [ 0.59215686  0.4627451   0.32941176]\n",
      "   [ 0.48235294  0.36078431  0.28235294]]]\n",
      "\n",
      "\n",
      " [[[ 0.60392157  0.69411765  0.73333333]\n",
      "   [ 0.49411765  0.5372549   0.53333333]\n",
      "   [ 0.41176471  0.40784314  0.37254902]\n",
      "   ..., \n",
      "   [ 0.35686275  0.37254902  0.27843137]\n",
      "   [ 0.34117647  0.35294118  0.27843137]\n",
      "   [ 0.30980392  0.31764706  0.2745098 ]]\n",
      "\n",
      "  [[ 0.54901961  0.62745098  0.6627451 ]\n",
      "   [ 0.56862745  0.6         0.60392157]\n",
      "   [ 0.49019608  0.49019608  0.4627451 ]\n",
      "   ..., \n",
      "   [ 0.37647059  0.38823529  0.30588235]\n",
      "   [ 0.30196078  0.31372549  0.24313725]\n",
      "   [ 0.27843137  0.28627451  0.23921569]]\n",
      "\n",
      "  [[ 0.54901961  0.60784314  0.64313725]\n",
      "   [ 0.54509804  0.57254902  0.58431373]\n",
      "   [ 0.45098039  0.45098039  0.43921569]\n",
      "   ..., \n",
      "   [ 0.30980392  0.32156863  0.25098039]\n",
      "   [ 0.26666667  0.2745098   0.21568627]\n",
      "   [ 0.2627451   0.27058824  0.21568627]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.68627451  0.65490196  0.65098039]\n",
      "   [ 0.61176471  0.60392157  0.62745098]\n",
      "   [ 0.60392157  0.62745098  0.66666667]\n",
      "   ..., \n",
      "   [ 0.16470588  0.13333333  0.14117647]\n",
      "   [ 0.23921569  0.20784314  0.22352941]\n",
      "   [ 0.36470588  0.3254902   0.35686275]]\n",
      "\n",
      "  [[ 0.64705882  0.60392157  0.50196078]\n",
      "   [ 0.61176471  0.59607843  0.50980392]\n",
      "   [ 0.62352941  0.63137255  0.55686275]\n",
      "   ..., \n",
      "   [ 0.40392157  0.36470588  0.37647059]\n",
      "   [ 0.48235294  0.44705882  0.47058824]\n",
      "   [ 0.51372549  0.4745098   0.51372549]]\n",
      "\n",
      "  [[ 0.63921569  0.58039216  0.47058824]\n",
      "   [ 0.61960784  0.58039216  0.47843137]\n",
      "   [ 0.63921569  0.61176471  0.52156863]\n",
      "   ..., \n",
      "   [ 0.56078431  0.52156863  0.54509804]\n",
      "   [ 0.56078431  0.5254902   0.55686275]\n",
      "   [ 0.56078431  0.52156863  0.56470588]]]\n",
      "\n",
      "\n",
      " [[[ 1.          1.          1.        ]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   [ 0.99215686  0.99215686  0.99215686]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   ..., \n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   ..., \n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.99607843  0.99607843  0.99607843]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.44313725  0.47058824  0.43921569]\n",
      "   [ 0.43529412  0.4627451   0.43529412]\n",
      "   [ 0.41176471  0.43921569  0.41568627]\n",
      "   ..., \n",
      "   [ 0.28235294  0.31764706  0.31372549]\n",
      "   [ 0.28235294  0.31372549  0.30980392]\n",
      "   [ 0.28235294  0.31372549  0.30980392]]\n",
      "\n",
      "  [[ 0.43529412  0.4627451   0.43137255]\n",
      "   [ 0.40784314  0.43529412  0.40784314]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   ..., \n",
      "   [ 0.26666667  0.29411765  0.28627451]\n",
      "   [ 0.2745098   0.29803922  0.29411765]\n",
      "   [ 0.30588235  0.32941176  0.32156863]]\n",
      "\n",
      "  [[ 0.41568627  0.44313725  0.41176471]\n",
      "   [ 0.38823529  0.41568627  0.38431373]\n",
      "   [ 0.37254902  0.4         0.36862745]\n",
      "   ..., \n",
      "   [ 0.30588235  0.33333333  0.3254902 ]\n",
      "   [ 0.30980392  0.33333333  0.3254902 ]\n",
      "   [ 0.31372549  0.3372549   0.32941176]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.27058824  0.27843137  0.20392157]\n",
      "   [ 0.24313725  0.24313725  0.19215686]\n",
      "   [ 0.22745098  0.22352941  0.18823529]\n",
      "   ..., \n",
      "   [ 0.4627451   0.49019608  0.31372549]\n",
      "   [ 0.4745098   0.49019608  0.28627451]\n",
      "   [ 0.48235294  0.50588235  0.29019608]]\n",
      "\n",
      "  [[ 0.2745098   0.27843137  0.19215686]\n",
      "   [ 0.23137255  0.22745098  0.18431373]\n",
      "   [ 0.2         0.19215686  0.16862745]\n",
      "   ..., \n",
      "   [ 0.48235294  0.48235294  0.3254902 ]\n",
      "   [ 0.45882353  0.47843137  0.29803922]\n",
      "   [ 0.41568627  0.43921569  0.25490196]]\n",
      "\n",
      "  [[ 0.28627451  0.28235294  0.18431373]\n",
      "   [ 0.25490196  0.24705882  0.17647059]\n",
      "   [ 0.20392157  0.19607843  0.17254902]\n",
      "   ..., \n",
      "   [ 0.47058824  0.46666667  0.30980392]\n",
      "   [ 0.45098039  0.45098039  0.27058824]\n",
      "   [ 0.42745098  0.43529412  0.26666667]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.76862745  0.82352941  0.37254902]\n",
      "   [ 0.79215686  0.82745098  0.42745098]\n",
      "   [ 0.76078431  0.78823529  0.41176471]\n",
      "   ..., \n",
      "   [ 0.77647059  0.84313725  0.35294118]\n",
      "   [ 0.81568627  0.85882353  0.42352941]\n",
      "   [ 0.83137255  0.88235294  0.43529412]]\n",
      "\n",
      "  [[ 0.74509804  0.80784314  0.36862745]\n",
      "   [ 0.74901961  0.80784314  0.37254902]\n",
      "   [ 0.74509804  0.80392157  0.36470588]\n",
      "   ..., \n",
      "   [ 0.77647059  0.83921569  0.36862745]\n",
      "   [ 0.76470588  0.81568627  0.37254902]\n",
      "   [ 0.80784314  0.8627451   0.41960784]]\n",
      "\n",
      "  [[ 0.71764706  0.76470588  0.36470588]\n",
      "   [ 0.72156863  0.77254902  0.37647059]\n",
      "   [ 0.71372549  0.75686275  0.36078431]\n",
      "   ..., \n",
      "   [ 0.75686275  0.81176471  0.35686275]\n",
      "   [ 0.74117647  0.8         0.34117647]\n",
      "   [ 0.77647059  0.82745098  0.39215686]]]\n",
      "\n",
      "\n",
      " [[[ 0.61960784  0.61960784  0.61960784]\n",
      "   [ 0.61960784  0.61960784  0.61568627]\n",
      "   [ 0.61960784  0.62352941  0.60392157]\n",
      "   ..., \n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61176471  0.59215686]]\n",
      "\n",
      "  [[ 0.61568627  0.61568627  0.61568627]\n",
      "   [ 0.61960784  0.61960784  0.61568627]\n",
      "   [ 0.61568627  0.61568627  0.60784314]\n",
      "   ..., \n",
      "   [ 0.61176471  0.61568627  0.6       ]\n",
      "   [ 0.61176471  0.61568627  0.59607843]\n",
      "   [ 0.61176471  0.61176471  0.59607843]]\n",
      "\n",
      "  [[ 0.61176471  0.61176471  0.61176471]\n",
      "   [ 0.61568627  0.61568627  0.61176471]\n",
      "   [ 0.61568627  0.61568627  0.60784314]\n",
      "   ..., \n",
      "   [ 0.60784314  0.60784314  0.6       ]\n",
      "   [ 0.60784314  0.60784314  0.6       ]\n",
      "   [ 0.60784314  0.60784314  0.6       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.22745098  0.26666667  0.26666667]\n",
      "   [ 0.24313725  0.28235294  0.29019608]\n",
      "   [ 0.21960784  0.25882353  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.24313725  0.2745098   0.30196078]\n",
      "   [ 0.18823529  0.22352941  0.23137255]\n",
      "   [ 0.22352941  0.25882353  0.26666667]]\n",
      "\n",
      "  [[ 0.20392157  0.24313725  0.23921569]\n",
      "   [ 0.23529412  0.27058824  0.27843137]\n",
      "   [ 0.16862745  0.20392157  0.20392157]\n",
      "   ..., \n",
      "   [ 0.25882353  0.29803922  0.30980392]\n",
      "   [ 0.21568627  0.25882353  0.2627451 ]\n",
      "   [ 0.23529412  0.2745098   0.29019608]]\n",
      "\n",
      "  [[ 0.19607843  0.22745098  0.23137255]\n",
      "   [ 0.2         0.21960784  0.21960784]\n",
      "   [ 0.20784314  0.23529412  0.23921569]\n",
      "   ..., \n",
      "   [ 0.18039216  0.21176471  0.21176471]\n",
      "   [ 0.22745098  0.25882353  0.26666667]\n",
      "   [ 0.23921569  0.28235294  0.29019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.76862745  0.67058824  0.51372549]\n",
      "   [ 0.76862745  0.6745098   0.51372549]\n",
      "   [ 0.75686275  0.65882353  0.50196078]\n",
      "   ..., \n",
      "   [ 0.69411765  0.6         0.50980392]\n",
      "   [ 0.67058824  0.57647059  0.48627451]\n",
      "   [ 0.64705882  0.55294118  0.45882353]]\n",
      "\n",
      "  [[ 0.77647059  0.67843137  0.52156863]\n",
      "   [ 0.77647059  0.67843137  0.52156863]\n",
      "   [ 0.76470588  0.6627451   0.50588235]\n",
      "   ..., \n",
      "   [ 0.69803922  0.60784314  0.51764706]\n",
      "   [ 0.6745098   0.58431373  0.49411765]\n",
      "   [ 0.65490196  0.56470588  0.47058824]]\n",
      "\n",
      "  [[ 0.76862745  0.67058824  0.51764706]\n",
      "   [ 0.76862745  0.67058824  0.51764706]\n",
      "   [ 0.75294118  0.65490196  0.50196078]\n",
      "   ..., \n",
      "   [ 0.69019608  0.60392157  0.51764706]\n",
      "   [ 0.66666667  0.58431373  0.49411765]\n",
      "   [ 0.64705882  0.56078431  0.4745098 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.7372549   0.61960784  0.40392157]\n",
      "   [ 0.7372549   0.62745098  0.40392157]\n",
      "   [ 0.74117647  0.62745098  0.4       ]\n",
      "   ..., \n",
      "   [ 0.8         0.74509804  0.69803922]\n",
      "   [ 0.78431373  0.74117647  0.70196078]\n",
      "   [ 0.76862745  0.71764706  0.68235294]]\n",
      "\n",
      "  [[ 0.74117647  0.62352941  0.40392157]\n",
      "   [ 0.74117647  0.62745098  0.40784314]\n",
      "   [ 0.74509804  0.63137255  0.4       ]\n",
      "   ..., \n",
      "   [ 0.80392157  0.74509804  0.68627451]\n",
      "   [ 0.78431373  0.73333333  0.69019608]\n",
      "   [ 0.77254902  0.71764706  0.67843137]]\n",
      "\n",
      "  [[ 0.72156863  0.60392157  0.39215686]\n",
      "   [ 0.72156863  0.61176471  0.39215686]\n",
      "   [ 0.72156863  0.60784314  0.38039216]\n",
      "   ..., \n",
      "   [ 0.78039216  0.71372549  0.64705882]\n",
      "   [ 0.76862745  0.70588235  0.65490196]\n",
      "   [ 0.75686275  0.69411765  0.65098039]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.1372549   0.09803922  0.10196078]\n",
      "   [ 0.10588235  0.08235294  0.08235294]\n",
      "   [ 0.09803922  0.07843137  0.0745098 ]\n",
      "   ..., \n",
      "   [ 0.51764706  0.50588235  0.50588235]\n",
      "   [ 0.52156863  0.4745098   0.45490196]\n",
      "   [ 0.49411765  0.45098039  0.44313725]]\n",
      "\n",
      "  [[ 0.24705882  0.21568627  0.19607843]\n",
      "   [ 0.1254902   0.10588235  0.08235294]\n",
      "   [ 0.06666667  0.05098039  0.03137255]\n",
      "   ..., \n",
      "   [ 0.4         0.37254902  0.34509804]\n",
      "   [ 0.41176471  0.34901961  0.29803922]\n",
      "   [ 0.39215686  0.3372549   0.30196078]]\n",
      "\n",
      "  [[ 0.38823529  0.35686275  0.32941176]\n",
      "   [ 0.19215686  0.17647059  0.14509804]\n",
      "   [ 0.05882353  0.04705882  0.01960784]\n",
      "   ..., \n",
      "   [ 0.18039216  0.16862745  0.15294118]\n",
      "   [ 0.20392157  0.16078431  0.13333333]\n",
      "   [ 0.20392157  0.17254902  0.16078431]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.65098039  0.64705882  0.67058824]\n",
      "   [ 0.64313725  0.63921569  0.65098039]\n",
      "   [ 0.64313725  0.64313725  0.64705882]\n",
      "   ..., \n",
      "   [ 0.67843137  0.6745098   0.66666667]\n",
      "   [ 0.66666667  0.66666667  0.65882353]\n",
      "   [ 0.65490196  0.65490196  0.65490196]]\n",
      "\n",
      "  [[ 0.6627451   0.65882353  0.69019608]\n",
      "   [ 0.6627451   0.65882353  0.67843137]\n",
      "   [ 0.65882353  0.65882353  0.67058824]\n",
      "   ..., \n",
      "   [ 0.6745098   0.67058824  0.66666667]\n",
      "   [ 0.65882353  0.65490196  0.65490196]\n",
      "   [ 0.64705882  0.64705882  0.65098039]]\n",
      "\n",
      "  [[ 0.67843137  0.6745098   0.70196078]\n",
      "   [ 0.68627451  0.68235294  0.69803922]\n",
      "   [ 0.67843137  0.67843137  0.68627451]\n",
      "   ..., \n",
      "   [ 0.66666667  0.65882353  0.6627451 ]\n",
      "   [ 0.65882353  0.65490196  0.65882353]\n",
      "   [ 0.65098039  0.65098039  0.65882353]]]\n",
      "\n",
      "\n",
      " [[[ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07058824  0.05098039  0.03921569]\n",
      "   ..., \n",
      "   [ 0.07843137  0.0627451   0.0627451 ]\n",
      "   [ 0.08235294  0.0627451   0.05490196]\n",
      "   [ 0.08235294  0.0627451   0.05098039]]\n",
      "\n",
      "  [[ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07058824  0.05098039  0.03921569]\n",
      "   ..., \n",
      "   [ 0.07843137  0.0627451   0.05882353]\n",
      "   [ 0.08235294  0.0627451   0.05098039]\n",
      "   [ 0.08235294  0.0627451   0.05098039]]\n",
      "\n",
      "  [[ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.07058824  0.05098039  0.03921569]\n",
      "   ..., \n",
      "   [ 0.07843137  0.0627451   0.05490196]\n",
      "   [ 0.08235294  0.0627451   0.05098039]\n",
      "   [ 0.08235294  0.0627451   0.05098039]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.25882353  0.21176471  0.16078431]\n",
      "   [ 0.31372549  0.2627451   0.20784314]\n",
      "   [ 0.18431373  0.1372549   0.0745098 ]\n",
      "   ..., \n",
      "   [ 0.5254902   0.5254902   0.39215686]\n",
      "   [ 0.43137255  0.44313725  0.30196078]\n",
      "   [ 0.38431373  0.4         0.25882353]]\n",
      "\n",
      "  [[ 0.23529412  0.18823529  0.12941176]\n",
      "   [ 0.21568627  0.16862745  0.10588235]\n",
      "   [ 0.19607843  0.14901961  0.08627451]\n",
      "   ..., \n",
      "   [ 0.48235294  0.49019608  0.3254902 ]\n",
      "   [ 0.30980392  0.31764706  0.16470588]\n",
      "   [ 0.28235294  0.29019608  0.14901961]]\n",
      "\n",
      "  [[ 0.25098039  0.21176471  0.14901961]\n",
      "   [ 0.21568627  0.17647059  0.11372549]\n",
      "   [ 0.18823529  0.14901961  0.08235294]\n",
      "   ..., \n",
      "   [ 0.60784314  0.61568627  0.43529412]\n",
      "   [ 0.53333333  0.5372549   0.38039216]\n",
      "   [ 0.34509804  0.34901961  0.2       ]]]\n",
      "\n",
      "\n",
      " [[[ 0.45490196  0.40392157  0.21960784]\n",
      "   [ 0.45098039  0.41176471  0.23137255]\n",
      "   [ 0.60784314  0.50196078  0.32156863]\n",
      "   ..., \n",
      "   [ 0.68627451  0.51764706  0.30196078]\n",
      "   [ 0.6627451   0.52156863  0.28235294]\n",
      "   [ 0.55686275  0.46666667  0.20784314]]\n",
      "\n",
      "  [[ 0.45490196  0.4         0.22745098]\n",
      "   [ 0.47843137  0.42352941  0.25490196]\n",
      "   [ 0.6         0.4745098   0.30980392]\n",
      "   ..., \n",
      "   [ 0.58823529  0.43529412  0.22352941]\n",
      "   [ 0.56862745  0.4745098   0.23529412]\n",
      "   [ 0.52156863  0.48235294  0.21176471]]\n",
      "\n",
      "  [[ 0.37254902  0.3372549   0.16078431]\n",
      "   [ 0.38431373  0.32941176  0.17254902]\n",
      "   [ 0.55294118  0.41568627  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.56862745  0.43921569  0.22745098]\n",
      "   [ 0.49411765  0.43529412  0.2       ]\n",
      "   [ 0.49803922  0.49019608  0.24313725]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.30196078  0.24705882  0.11372549]\n",
      "   [ 0.34509804  0.28235294  0.14509804]\n",
      "   [ 0.2745098   0.23137255  0.10588235]\n",
      "   ..., \n",
      "   [ 0.18823529  0.15294118  0.07843137]\n",
      "   [ 0.45490196  0.42352941  0.32941176]\n",
      "   [ 0.62352941  0.55686275  0.47843137]]\n",
      "\n",
      "  [[ 0.21568627  0.14509804  0.0627451 ]\n",
      "   [ 0.25490196  0.18039216  0.09411765]\n",
      "   [ 0.26666667  0.20784314  0.11764706]\n",
      "   ..., \n",
      "   [ 0.16470588  0.11764706  0.05098039]\n",
      "   [ 0.49411765  0.44705882  0.35294118]\n",
      "   [ 0.62745098  0.57647059  0.49019608]]\n",
      "\n",
      "  [[ 0.30588235  0.22352941  0.14509804]\n",
      "   [ 0.28235294  0.19607843  0.11764706]\n",
      "   [ 0.2627451   0.19607843  0.1254902 ]\n",
      "   ..., \n",
      "   [ 0.20392157  0.14509804  0.07058824]\n",
      "   [ 0.48627451  0.43137255  0.32941176]\n",
      "   [ 0.60784314  0.56470588  0.48627451]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.41568627  0.3372549   0.61176471]\n",
      "   [ 0.48235294  0.4         0.50588235]\n",
      "   [ 0.43529412  0.33333333  0.49019608]\n",
      "   ..., \n",
      "   [ 0.51372549  0.4627451   0.61568627]\n",
      "   [ 0.42745098  0.39215686  0.55686275]\n",
      "   [ 0.55294118  0.52941176  0.61960784]]\n",
      "\n",
      "  [[ 0.60784314  0.5254902   0.63529412]\n",
      "   [ 0.6745098   0.57254902  0.44313725]\n",
      "   [ 0.51372549  0.43529412  0.48627451]\n",
      "   ..., \n",
      "   [ 0.51764706  0.46666667  0.60784314]\n",
      "   [ 0.39215686  0.34901961  0.50980392]\n",
      "   [ 0.56862745  0.5372549   0.63529412]]\n",
      "\n",
      "  [[ 0.63529412  0.5254902   0.68235294]\n",
      "   [ 0.62745098  0.49803922  0.43137255]\n",
      "   [ 0.44313725  0.34509804  0.5254902 ]\n",
      "   ..., \n",
      "   [ 0.73333333  0.71372549  0.65490196]\n",
      "   [ 0.69411765  0.6745098   0.65882353]\n",
      "   [ 0.78823529  0.77254902  0.71764706]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.35686275  0.03921569  0.5254902 ]\n",
      "   [ 0.34509804  0.03529412  0.49803922]\n",
      "   [ 0.34509804  0.05098039  0.49803922]\n",
      "   ..., \n",
      "   [ 0.40784314  0.24705882  0.49411765]\n",
      "   [ 0.43529412  0.28235294  0.52156863]\n",
      "   [ 0.43921569  0.25490196  0.53333333]]\n",
      "\n",
      "  [[ 0.29803922  0.02352941  0.50196078]\n",
      "   [ 0.30588235  0.01568627  0.47058824]\n",
      "   [ 0.35686275  0.08235294  0.51764706]\n",
      "   ..., \n",
      "   [ 0.42745098  0.27058824  0.49803922]\n",
      "   [ 0.41960784  0.26666667  0.50980392]\n",
      "   [ 0.44705882  0.24705882  0.54509804]]\n",
      "\n",
      "  [[ 0.2627451   0.03137255  0.50196078]\n",
      "   [ 0.26666667  0.02352941  0.47058824]\n",
      "   [ 0.30980392  0.07058824  0.49803922]\n",
      "   ..., \n",
      "   [ 0.45882353  0.28627451  0.5254902 ]\n",
      "   [ 0.43137255  0.25098039  0.52156863]\n",
      "   [ 0.38823529  0.16470588  0.49019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.96862745  0.96470588  0.97254902]\n",
      "   [ 0.96078431  0.97647059  0.98823529]\n",
      "   [ 0.95686275  0.97254902  0.97647059]\n",
      "   ..., \n",
      "   [ 0.45882353  0.38039216  0.32941176]\n",
      "   [ 0.49803922  0.41960784  0.37647059]\n",
      "   [ 0.61960784  0.56470588  0.54117647]]\n",
      "\n",
      "  [[ 0.95294118  0.95686275  0.96862745]\n",
      "   [ 0.95294118  0.96862745  0.97647059]\n",
      "   [ 0.95294118  0.96078431  0.95686275]\n",
      "   ..., \n",
      "   [ 0.44313725  0.35686275  0.29803922]\n",
      "   [ 0.47843137  0.4         0.34509804]\n",
      "   [ 0.63137255  0.56862745  0.52941176]]\n",
      "\n",
      "  [[ 0.95686275  0.96078431  0.97647059]\n",
      "   [ 0.96078431  0.97647059  0.98039216]\n",
      "   [ 0.96862745  0.96862745  0.95686275]\n",
      "   ..., \n",
      "   [ 0.52941176  0.42745098  0.36470588]\n",
      "   [ 0.4745098   0.37647059  0.31764706]\n",
      "   [ 0.50588235  0.43137255  0.38823529]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.71372549  0.6627451   0.63529412]\n",
      "   [ 0.77647059  0.7254902   0.69803922]\n",
      "   [ 0.85490196  0.80392157  0.77647059]\n",
      "   ..., \n",
      "   [ 0.61568627  0.54901961  0.44313725]\n",
      "   [ 0.36862745  0.30588235  0.21960784]\n",
      "   [ 0.50980392  0.45882353  0.41176471]]\n",
      "\n",
      "  [[ 0.54509804  0.4627451   0.41960784]\n",
      "   [ 0.49411765  0.40392157  0.35686275]\n",
      "   [ 0.5254902   0.43529412  0.38431373]\n",
      "   ..., \n",
      "   [ 0.61960784  0.55686275  0.43921569]\n",
      "   [ 0.46666667  0.40784314  0.31372549]\n",
      "   [ 0.45490196  0.40784314  0.35294118]]\n",
      "\n",
      "  [[ 0.76862745  0.70196078  0.62745098]\n",
      "   [ 0.7254902   0.63921569  0.54509804]\n",
      "   [ 0.69019608  0.58823529  0.48235294]\n",
      "   ..., \n",
      "   [ 0.59607843  0.54901961  0.42352941]\n",
      "   [ 0.69411765  0.64313725  0.5372549 ]\n",
      "   [ 0.63529412  0.59607843  0.52156863]]]\n",
      "\n",
      "\n",
      " [[[ 0.74117647  0.89803922  0.94117647]\n",
      "   [ 0.76470588  0.90980392  0.94901961]\n",
      "   [ 0.79607843  0.93333333  0.96470588]\n",
      "   ..., \n",
      "   [ 0.65882353  0.75686275  0.81176471]\n",
      "   [ 0.65882353  0.74901961  0.79215686]\n",
      "   [ 0.65882353  0.7372549   0.78039216]]\n",
      "\n",
      "  [[ 0.79607843  0.9372549   0.96470588]\n",
      "   [ 0.83137255  0.96470588  0.98431373]\n",
      "   [ 0.82352941  0.94901961  0.96862745]\n",
      "   ..., \n",
      "   [ 0.58431373  0.67843137  0.76470588]\n",
      "   [ 0.59215686  0.6745098   0.76470588]\n",
      "   [ 0.60784314  0.67843137  0.77254902]]\n",
      "\n",
      "  [[ 0.83529412  0.96862745  0.98431373]\n",
      "   [ 0.81568627  0.94509804  0.96078431]\n",
      "   [ 0.82745098  0.94509804  0.95686275]\n",
      "   ..., \n",
      "   [ 0.58431373  0.6745098   0.76862745]\n",
      "   [ 0.57647059  0.65490196  0.76470588]\n",
      "   [ 0.56470588  0.63137255  0.75686275]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.29019608  0.31764706  0.35294118]\n",
      "   [ 0.21176471  0.32156863  0.47843137]\n",
      "   [ 0.15294118  0.40784314  0.62352941]\n",
      "   ..., \n",
      "   [ 0.18039216  0.15294118  0.18039216]\n",
      "   [ 0.19607843  0.16862745  0.2       ]\n",
      "   [ 0.27058824  0.24705882  0.25098039]]\n",
      "\n",
      "  [[ 0.25490196  0.2627451   0.28627451]\n",
      "   [ 0.16470588  0.22745098  0.35686275]\n",
      "   [ 0.17647059  0.36078431  0.50980392]\n",
      "   ..., \n",
      "   [ 0.14509804  0.08627451  0.17647059]\n",
      "   [ 0.17254902  0.10588235  0.18431373]\n",
      "   [ 0.22352941  0.17254902  0.2       ]]\n",
      "\n",
      "  [[ 0.20784314  0.22745098  0.24705882]\n",
      "   [ 0.15294118  0.2         0.25490196]\n",
      "   [ 0.24705882  0.30588235  0.36862745]\n",
      "   ..., \n",
      "   [ 0.18039216  0.10196078  0.19215686]\n",
      "   [ 0.23137255  0.14901961  0.18431373]\n",
      "   [ 0.29411765  0.23921569  0.2       ]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.10196078  0.09019608  0.1254902 ]\n",
      "   [ 0.06666667  0.05490196  0.09803922]\n",
      "   [ 0.05098039  0.03529412  0.09411765]\n",
      "   ..., \n",
      "   [ 0.05882353  0.05490196  0.10980392]\n",
      "   [ 0.09411765  0.09411765  0.14509804]\n",
      "   [ 0.08627451  0.08235294  0.13333333]]\n",
      "\n",
      "  [[ 0.07843137  0.06666667  0.10196078]\n",
      "   [ 0.05098039  0.03921569  0.08627451]\n",
      "   [ 0.05098039  0.03529412  0.09411765]\n",
      "   ..., \n",
      "   [ 0.0745098   0.06666667  0.1372549 ]\n",
      "   [ 0.08235294  0.07843137  0.1372549 ]\n",
      "   [ 0.11372549  0.11372549  0.15294118]]\n",
      "\n",
      "  [[ 0.05490196  0.04313725  0.07843137]\n",
      "   [ 0.05098039  0.03921569  0.08235294]\n",
      "   [ 0.05098039  0.03529412  0.09019608]\n",
      "   ..., \n",
      "   [ 0.06666667  0.0627451   0.1254902 ]\n",
      "   [ 0.09803922  0.09411765  0.14901961]\n",
      "   [ 0.12156863  0.12156863  0.16470588]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.35294118  0.42745098  0.5372549 ]\n",
      "   [ 0.13333333  0.25098039  0.37254902]\n",
      "   [ 0.10980392  0.21176471  0.35294118]\n",
      "   ..., \n",
      "   [ 0.09019608  0.07843137  0.14509804]\n",
      "   [ 0.0627451   0.05098039  0.11764706]\n",
      "   [ 0.03529412  0.02352941  0.09019608]]\n",
      "\n",
      "  [[ 0.30980392  0.41176471  0.55294118]\n",
      "   [ 0.22745098  0.37647059  0.54509804]\n",
      "   [ 0.1254902   0.26666667  0.43137255]\n",
      "   ..., \n",
      "   [ 0.05490196  0.04313725  0.10980392]\n",
      "   [ 0.0627451   0.05098039  0.11764706]\n",
      "   [ 0.03921569  0.02745098  0.09411765]]\n",
      "\n",
      "  [[ 0.50196078  0.61568627  0.76862745]\n",
      "   [ 0.22745098  0.36470588  0.58431373]\n",
      "   [ 0.09803922  0.23529412  0.41568627]\n",
      "   ..., \n",
      "   [ 0.05098039  0.03921569  0.10588235]\n",
      "   [ 0.04705882  0.03529412  0.10196078]\n",
      "   [ 0.05098039  0.03921569  0.10588235]]]\n",
      "\n",
      "\n",
      " [[[ 0.36862745  0.3372549   0.22745098]\n",
      "   [ 0.39607843  0.35686275  0.23921569]\n",
      "   [ 0.37254902  0.33333333  0.21176471]\n",
      "   ..., \n",
      "   [ 0.56862745  0.54117647  0.41568627]\n",
      "   [ 0.56862745  0.54901961  0.42352941]\n",
      "   [ 0.4745098   0.45882353  0.35294118]]\n",
      "\n",
      "  [[ 0.34901961  0.32941176  0.21568627]\n",
      "   [ 0.38039216  0.34901961  0.23137255]\n",
      "   [ 0.39607843  0.35686275  0.23529412]\n",
      "   ..., \n",
      "   [ 0.57254902  0.5372549   0.41568627]\n",
      "   [ 0.57254902  0.54509804  0.41960784]\n",
      "   [ 0.47843137  0.45882353  0.35294118]]\n",
      "\n",
      "  [[ 0.3372549   0.32941176  0.21176471]\n",
      "   [ 0.36862745  0.34509804  0.22352941]\n",
      "   [ 0.41960784  0.38431373  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.57254902  0.53333333  0.41568627]\n",
      "   [ 0.57647059  0.54117647  0.41960784]\n",
      "   [ 0.48235294  0.45490196  0.35294118]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.80392157  0.80392157  0.8       ]\n",
      "   [ 0.81568627  0.81568627  0.81176471]\n",
      "   [ 0.78823529  0.78823529  0.78431373]\n",
      "   ..., \n",
      "   [ 0.56862745  0.58431373  0.58431373]\n",
      "   [ 0.58431373  0.59607843  0.61568627]\n",
      "   [ 0.49019608  0.50196078  0.52941176]]\n",
      "\n",
      "  [[ 0.78823529  0.78823529  0.78823529]\n",
      "   [ 0.80392157  0.80392157  0.80392157]\n",
      "   [ 0.77647059  0.77647059  0.77647059]\n",
      "   ..., \n",
      "   [ 0.60392157  0.61960784  0.6627451 ]\n",
      "   [ 0.61960784  0.63529412  0.69411765]\n",
      "   [ 0.5254902   0.54117647  0.6       ]]\n",
      "\n",
      "  [[ 0.74509804  0.74509804  0.74117647]\n",
      "   [ 0.7372549   0.7372549   0.73333333]\n",
      "   [ 0.68627451  0.68627451  0.67843137]\n",
      "   ..., \n",
      "   [ 0.63529412  0.64313725  0.71372549]\n",
      "   [ 0.63921569  0.65098039  0.72156863]\n",
      "   [ 0.52941176  0.5372549   0.60784314]]]\n",
      "\n",
      "\n",
      " [[[ 0.71764706  0.72941176  0.69803922]\n",
      "   [ 0.61960784  0.65490196  0.59607843]\n",
      "   [ 0.65098039  0.6745098   0.62745098]\n",
      "   ..., \n",
      "   [ 0.55686275  0.57647059  0.56470588]\n",
      "   [ 0.5372549   0.57254902  0.55294118]\n",
      "   [ 0.56470588  0.58823529  0.57254902]]\n",
      "\n",
      "  [[ 0.49019608  0.5254902   0.47058824]\n",
      "   [ 0.38039216  0.43529412  0.35294118]\n",
      "   [ 0.39215686  0.43921569  0.37254902]\n",
      "   ..., \n",
      "   [ 0.24705882  0.29019608  0.26666667]\n",
      "   [ 0.23137255  0.29019608  0.2627451 ]\n",
      "   [ 0.2627451   0.30980392  0.28235294]]\n",
      "\n",
      "  [[ 0.41960784  0.4745098   0.40392157]\n",
      "   [ 0.33333333  0.40784314  0.30980392]\n",
      "   [ 0.34509804  0.40784314  0.3254902 ]\n",
      "   ..., \n",
      "   [ 0.24705882  0.30980392  0.27843137]\n",
      "   [ 0.20784314  0.28627451  0.25098039]\n",
      "   [ 0.23529412  0.30196078  0.27058824]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.42352941  0.45490196  0.40392157]\n",
      "   [ 0.38431373  0.41960784  0.34117647]\n",
      "   [ 0.46666667  0.50980392  0.41176471]\n",
      "   ..., \n",
      "   [ 0.44705882  0.49019608  0.44313725]\n",
      "   [ 0.40784314  0.45098039  0.40392157]\n",
      "   [ 0.35686275  0.39607843  0.35294118]]\n",
      "\n",
      "  [[ 0.78039216  0.80392157  0.77254902]\n",
      "   [ 0.77254902  0.79607843  0.74509804]\n",
      "   [ 0.81176471  0.83921569  0.76862745]\n",
      "   ..., \n",
      "   [ 0.79607843  0.81960784  0.79215686]\n",
      "   [ 0.78431373  0.80392157  0.78039216]\n",
      "   [ 0.74117647  0.76078431  0.7372549 ]]\n",
      "\n",
      "  [[ 0.98431373  1.          0.98823529]\n",
      "   [ 0.97254902  0.98823529  0.96470588]\n",
      "   [ 0.97647059  0.99215686  0.95686275]\n",
      "   ..., \n",
      "   [ 0.98039216  0.98431373  0.98039216]\n",
      "   [ 0.98039216  0.98039216  0.98039216]\n",
      "   [ 0.98039216  0.98431373  0.98039216]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.6745098   0.67843137  0.59607843]\n",
      "   [ 0.67058824  0.6745098   0.59607843]\n",
      "   [ 0.69803922  0.69803922  0.61960784]\n",
      "   ..., \n",
      "   [ 0.41960784  0.41176471  0.30196078]\n",
      "   [ 0.41568627  0.40392157  0.30980392]\n",
      "   [ 0.4         0.38823529  0.30588235]]\n",
      "\n",
      "  [[ 0.64705882  0.63921569  0.56078431]\n",
      "   [ 0.62352941  0.61568627  0.54117647]\n",
      "   [ 0.70588235  0.69803922  0.62352941]\n",
      "   ..., \n",
      "   [ 0.45882353  0.43921569  0.3254902 ]\n",
      "   [ 0.45882353  0.43921569  0.3372549 ]\n",
      "   [ 0.43529412  0.41568627  0.3254902 ]]\n",
      "\n",
      "  [[ 0.68235294  0.6627451   0.58823529]\n",
      "   [ 0.61176471  0.59215686  0.51764706]\n",
      "   [ 0.68235294  0.6627451   0.58823529]\n",
      "   ..., \n",
      "   [ 0.47058824  0.44705882  0.3254902 ]\n",
      "   [ 0.4745098   0.44705882  0.3372549 ]\n",
      "   [ 0.46666667  0.43921569  0.34509804]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.47843137  0.45882353  0.36862745]\n",
      "   [ 0.47058824  0.45098039  0.36470588]\n",
      "   [ 0.45490196  0.43921569  0.34901961]\n",
      "   ..., \n",
      "   [ 0.48627451  0.49803922  0.39215686]\n",
      "   [ 0.4745098   0.49411765  0.39215686]\n",
      "   [ 0.45882353  0.47843137  0.38039216]]\n",
      "\n",
      "  [[ 0.43529412  0.41176471  0.30588235]\n",
      "   [ 0.43921569  0.41960784  0.3254902 ]\n",
      "   [ 0.48235294  0.48627451  0.39607843]\n",
      "   ..., \n",
      "   [ 0.47843137  0.45490196  0.32941176]\n",
      "   [ 0.45882353  0.4627451   0.35294118]\n",
      "   [ 0.44313725  0.45882353  0.35294118]]\n",
      "\n",
      "  [[ 0.43921569  0.41176471  0.30196078]\n",
      "   [ 0.45098039  0.42352941  0.32941176]\n",
      "   [ 0.4627451   0.47058824  0.37647059]\n",
      "   ..., \n",
      "   [ 0.48627451  0.45098039  0.31372549]\n",
      "   [ 0.43137255  0.43137255  0.31764706]\n",
      "   [ 0.4         0.41568627  0.30980392]]]\n",
      "\n",
      "\n",
      " [[[ 0.37254902  0.34509804  0.2       ]\n",
      "   [ 0.36078431  0.34509804  0.19215686]\n",
      "   [ 0.35686275  0.34117647  0.19607843]\n",
      "   ..., \n",
      "   [ 0.20784314  0.20392157  0.11764706]\n",
      "   [ 0.20392157  0.2         0.11764706]\n",
      "   [ 0.21176471  0.21176471  0.1254902 ]]\n",
      "\n",
      "  [[ 0.4         0.36862745  0.20784314]\n",
      "   [ 0.41568627  0.38039216  0.2       ]\n",
      "   [ 0.41960784  0.38431373  0.21176471]\n",
      "   ..., \n",
      "   [ 0.20392157  0.20392157  0.1254902 ]\n",
      "   [ 0.19215686  0.18823529  0.10588235]\n",
      "   [ 0.23137255  0.22745098  0.1254902 ]]\n",
      "\n",
      "  [[ 0.41960784  0.38431373  0.20392157]\n",
      "   [ 0.42745098  0.38823529  0.20392157]\n",
      "   [ 0.41176471  0.38039216  0.20392157]\n",
      "   ..., \n",
      "   [ 0.20784314  0.20784314  0.1254902 ]\n",
      "   [ 0.21568627  0.20784314  0.11764706]\n",
      "   [ 0.24313725  0.23529412  0.1254902 ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.37647059  0.36078431  0.23529412]\n",
      "   [ 0.24313725  0.23921569  0.15686275]\n",
      "   [ 0.20784314  0.21176471  0.14117647]\n",
      "   ..., \n",
      "   [ 0.19607843  0.19607843  0.12941176]\n",
      "   [ 0.21568627  0.21568627  0.14117647]\n",
      "   [ 0.23529412  0.22745098  0.15294118]]\n",
      "\n",
      "  [[ 0.31372549  0.29803922  0.2       ]\n",
      "   [ 0.23137255  0.22745098  0.15294118]\n",
      "   [ 0.19607843  0.2         0.12941176]\n",
      "   ..., \n",
      "   [ 0.21960784  0.20392157  0.13333333]\n",
      "   [ 0.25098039  0.23529412  0.15294118]\n",
      "   [ 0.25098039  0.23529412  0.14901961]]\n",
      "\n",
      "  [[ 0.18431373  0.18039216  0.1254902 ]\n",
      "   [ 0.15294118  0.15686275  0.10980392]\n",
      "   [ 0.15294118  0.15686275  0.09803922]\n",
      "   ..., \n",
      "   [ 0.30980392  0.29019608  0.17647059]\n",
      "   [ 0.25098039  0.23137255  0.14901961]\n",
      "   [ 0.23921569  0.22352941  0.14901961]]]\n",
      "\n",
      "\n",
      " [[[ 0.60784314  0.40392157  0.43137255]\n",
      "   [ 0.59607843  0.39215686  0.41960784]\n",
      "   [ 0.60784314  0.40392157  0.43137255]\n",
      "   ..., \n",
      "   [ 0.23137255  0.14901961  0.17254902]\n",
      "   [ 0.22352941  0.15686275  0.18039216]\n",
      "   [ 0.22352941  0.16078431  0.19607843]]\n",
      "\n",
      "  [[ 0.60784314  0.40784314  0.43529412]\n",
      "   [ 0.58039216  0.38039216  0.40784314]\n",
      "   [ 0.6         0.4         0.42745098]\n",
      "   ..., \n",
      "   [ 0.28627451  0.18823529  0.20784314]\n",
      "   [ 0.21960784  0.15294118  0.17647059]\n",
      "   [ 0.21960784  0.16078431  0.19215686]]\n",
      "\n",
      "  [[ 0.61176471  0.41176471  0.43921569]\n",
      "   [ 0.58823529  0.38823529  0.41568627]\n",
      "   [ 0.58039216  0.38039216  0.40784314]\n",
      "   ..., \n",
      "   [ 0.41176471  0.30980392  0.3254902 ]\n",
      "   [ 0.23137255  0.16470588  0.18823529]\n",
      "   [ 0.2         0.14117647  0.17254902]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.45490196  0.43921569  0.5254902 ]\n",
      "   [ 0.44313725  0.43529412  0.50980392]\n",
      "   [ 0.44313725  0.44313725  0.50196078]\n",
      "   ..., \n",
      "   [ 0.42745098  0.38039216  0.44705882]\n",
      "   [ 0.28235294  0.23921569  0.29803922]\n",
      "   [ 0.42352941  0.37647059  0.43529412]]\n",
      "\n",
      "  [[ 0.45490196  0.44313725  0.52156863]\n",
      "   [ 0.44705882  0.43529412  0.50980392]\n",
      "   [ 0.45098039  0.43921569  0.51372549]\n",
      "   ..., \n",
      "   [ 0.43137255  0.41176471  0.48627451]\n",
      "   [ 0.23137255  0.19607843  0.26666667]\n",
      "   [ 0.29411765  0.23921569  0.30980392]]\n",
      "\n",
      "  [[ 0.46666667  0.45490196  0.52941176]\n",
      "   [ 0.45490196  0.44313725  0.51764706]\n",
      "   [ 0.45490196  0.44313725  0.51764706]\n",
      "   ..., \n",
      "   [ 0.47058824  0.45882353  0.5372549 ]\n",
      "   [ 0.39607843  0.37254902  0.44705882]\n",
      "   [ 0.24705882  0.19607843  0.26666667]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.69803922  0.69019608  0.74117647]\n",
      "   [ 0.69803922  0.69019608  0.74117647]\n",
      "   [ 0.69803922  0.69019608  0.74117647]\n",
      "   ..., \n",
      "   [ 0.66666667  0.65882353  0.70588235]\n",
      "   [ 0.65882353  0.65098039  0.69411765]\n",
      "   [ 0.64705882  0.63921569  0.68235294]]\n",
      "\n",
      "  [[ 0.70588235  0.69803922  0.74901961]\n",
      "   [ 0.70196078  0.69411765  0.74509804]\n",
      "   [ 0.70588235  0.69803922  0.74901961]\n",
      "   ..., \n",
      "   [ 0.67843137  0.67058824  0.71372549]\n",
      "   [ 0.67058824  0.6627451   0.70588235]\n",
      "   [ 0.65882353  0.65098039  0.69411765]]\n",
      "\n",
      "  [[ 0.69411765  0.68627451  0.7372549 ]\n",
      "   [ 0.69411765  0.68627451  0.7372549 ]\n",
      "   [ 0.69803922  0.69019608  0.74117647]\n",
      "   ..., \n",
      "   [ 0.67058824  0.6627451   0.70588235]\n",
      "   [ 0.6627451   0.65490196  0.69803922]\n",
      "   [ 0.65490196  0.64705882  0.69019608]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.43921569  0.41960784  0.41960784]\n",
      "   [ 0.44313725  0.42745098  0.42352941]\n",
      "   [ 0.44705882  0.43137255  0.43137255]\n",
      "   ..., \n",
      "   [ 0.39215686  0.38039216  0.36862745]\n",
      "   [ 0.38431373  0.36862745  0.36470588]\n",
      "   [ 0.39607843  0.37254902  0.37254902]]\n",
      "\n",
      "  [[ 0.43921569  0.4         0.39607843]\n",
      "   [ 0.43921569  0.40392157  0.4       ]\n",
      "   [ 0.44313725  0.40392157  0.40392157]\n",
      "   ..., \n",
      "   [ 0.4         0.37254902  0.36470588]\n",
      "   [ 0.4         0.36470588  0.35686275]\n",
      "   [ 0.4         0.36078431  0.35686275]]\n",
      "\n",
      "  [[ 0.40392157  0.37647059  0.36078431]\n",
      "   [ 0.39215686  0.36470588  0.35294118]\n",
      "   [ 0.40392157  0.37254902  0.36862745]\n",
      "   ..., \n",
      "   [ 0.36078431  0.32941176  0.31372549]\n",
      "   [ 0.36470588  0.3372549   0.31372549]\n",
      "   [ 0.35686275  0.32941176  0.30196078]]]\n",
      "\n",
      "\n",
      " [[[ 0.11372549  0.16862745  0.03921569]\n",
      "   [ 0.08627451  0.14117647  0.01568627]\n",
      "   [ 0.09803922  0.14509804  0.0627451 ]\n",
      "   ..., \n",
      "   [ 0.77254902  0.85882353  0.5372549 ]\n",
      "   [ 0.77647059  0.85882353  0.5372549 ]\n",
      "   [ 0.78039216  0.87058824  0.54901961]]\n",
      "\n",
      "  [[ 0.12156863  0.18039216  0.03529412]\n",
      "   [ 0.10588235  0.16078431  0.02352941]\n",
      "   [ 0.06666667  0.11372549  0.02352941]\n",
      "   ..., \n",
      "   [ 0.82352941  0.90980392  0.58039216]\n",
      "   [ 0.81960784  0.90588235  0.58039216]\n",
      "   [ 0.81960784  0.90588235  0.58039216]]\n",
      "\n",
      "  [[ 0.15686275  0.21568627  0.0627451 ]\n",
      "   [ 0.12156863  0.17647059  0.03137255]\n",
      "   [ 0.07843137  0.12941176  0.02745098]\n",
      "   ..., \n",
      "   [ 0.82352941  0.90980392  0.58823529]\n",
      "   [ 0.82352941  0.90980392  0.58431373]\n",
      "   [ 0.82352941  0.90980392  0.58431373]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.17647059  0.14901961  0.09019608]\n",
      "   [ 0.09411765  0.08235294  0.04313725]\n",
      "   [ 0.0627451   0.05490196  0.02745098]\n",
      "   ..., \n",
      "   [ 0.09803922  0.11372549  0.1254902 ]\n",
      "   [ 0.09411765  0.10980392  0.12156863]\n",
      "   [ 0.09411765  0.10980392  0.12156863]]\n",
      "\n",
      "  [[ 0.08235294  0.07058824  0.02745098]\n",
      "   [ 0.07058824  0.05098039  0.01176471]\n",
      "   [ 0.10588235  0.0627451   0.01960784]\n",
      "   ..., \n",
      "   [ 0.10196078  0.11764706  0.12941176]\n",
      "   [ 0.11372549  0.12941176  0.14117647]\n",
      "   [ 0.10980392  0.1254902   0.1372549 ]]\n",
      "\n",
      "  [[ 0.20784314  0.15686275  0.09019608]\n",
      "   [ 0.31764706  0.24313725  0.14901961]\n",
      "   [ 0.38039216  0.2745098   0.16862745]\n",
      "   ..., \n",
      "   [ 0.08627451  0.10196078  0.11372549]\n",
      "   [ 0.09411765  0.10980392  0.12156863]\n",
      "   [ 0.09019608  0.10588235  0.11764706]]]\n",
      "\n",
      "\n",
      " [[[ 0.14117647  0.25490196  0.4       ]\n",
      "   [ 0.12941176  0.21568627  0.42352941]\n",
      "   [ 0.08235294  0.18431373  0.4627451 ]\n",
      "   ..., \n",
      "   [ 0.10196078  0.1254902   0.15686275]\n",
      "   [ 0.10196078  0.12156863  0.12156863]\n",
      "   [ 0.11372549  0.11372549  0.12156863]]\n",
      "\n",
      "  [[ 0.21568627  0.41960784  0.47058824]\n",
      "   [ 0.18431373  0.36862745  0.42352941]\n",
      "   [ 0.05882353  0.24705882  0.44313725]\n",
      "   ..., \n",
      "   [ 0.08627451  0.2         0.41176471]\n",
      "   [ 0.09019608  0.19215686  0.39215686]\n",
      "   [ 0.08235294  0.18039216  0.38039216]]\n",
      "\n",
      "  [[ 0.32156863  0.45490196  0.44705882]\n",
      "   [ 0.36862745  0.49803922  0.4       ]\n",
      "   [ 0.30980392  0.45882353  0.42352941]\n",
      "   ..., \n",
      "   [ 0.18431373  0.3254902   0.6       ]\n",
      "   [ 0.18431373  0.3372549   0.61176471]\n",
      "   [ 0.17647059  0.33333333  0.6       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.62352941  0.62745098  0.58039216]\n",
      "   [ 0.63529412  0.62352941  0.58431373]\n",
      "   [ 0.65098039  0.62745098  0.59215686]\n",
      "   ..., \n",
      "   [ 0.73333333  0.70588235  0.69411765]\n",
      "   [ 0.7254902   0.69019608  0.68235294]\n",
      "   [ 0.71764706  0.68235294  0.6745098 ]]\n",
      "\n",
      "  [[ 0.65098039  0.6627451   0.62745098]\n",
      "   [ 0.66666667  0.6627451   0.63137255]\n",
      "   [ 0.67843137  0.6627451   0.63529412]\n",
      "   ..., \n",
      "   [ 0.71764706  0.69019608  0.68235294]\n",
      "   [ 0.70980392  0.67843137  0.67058824]\n",
      "   [ 0.70588235  0.6745098   0.67058824]]\n",
      "\n",
      "  [[ 0.65882353  0.68235294  0.65098039]\n",
      "   [ 0.67058824  0.67843137  0.65490196]\n",
      "   [ 0.68627451  0.67843137  0.65882353]\n",
      "   ..., \n",
      "   [ 0.71372549  0.69019608  0.67843137]\n",
      "   [ 0.70980392  0.6745098   0.66666667]\n",
      "   [ 0.70588235  0.6745098   0.66666667]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.27058824  0.34509804  0.44705882]\n",
      "   [ 0.35294118  0.4745098   0.58823529]\n",
      "   [ 0.35294118  0.49803922  0.61960784]\n",
      "   ..., \n",
      "   [ 0.00784314  0.01176471  0.07058824]\n",
      "   [ 0.00784314  0.00784314  0.0627451 ]\n",
      "   [ 0.00784314  0.00784314  0.05882353]]\n",
      "\n",
      "  [[ 0.11372549  0.15294118  0.25098039]\n",
      "   [ 0.05882353  0.10588235  0.20784314]\n",
      "   [ 0.05098039  0.09803922  0.2       ]\n",
      "   ..., \n",
      "   [ 0.00392157  0.          0.00784314]\n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.00392157  0.00784314]]\n",
      "\n",
      "  [[ 0.01568627  0.01176471  0.01960784]\n",
      "   [ 0.01568627  0.00392157  0.00784314]\n",
      "   [ 0.01176471  0.00392157  0.00784314]\n",
      "   ..., \n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.          0.00784314]\n",
      "   [ 0.          0.          0.01176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.00392157  0.00392157  0.01960784]\n",
      "   [ 0.          0.          0.00392157]\n",
      "   [ 0.          0.          0.        ]\n",
      "   ..., \n",
      "   [ 0.03921569  0.03137255  0.04313725]\n",
      "   [ 0.00784314  0.00784314  0.01568627]\n",
      "   [ 0.00392157  0.00392157  0.01568627]]\n",
      "\n",
      "  [[ 0.03137255  0.0627451   0.12941176]\n",
      "   [ 0.00392157  0.01568627  0.05098039]\n",
      "   [ 0.          0.00392157  0.01176471]\n",
      "   ..., \n",
      "   [ 0.02745098  0.03921569  0.09019608]\n",
      "   [ 0.01176471  0.01568627  0.03921569]\n",
      "   [ 0.00392157  0.00392157  0.01176471]]\n",
      "\n",
      "  [[ 0.14509804  0.25098039  0.42352941]\n",
      "   [ 0.09411765  0.16862745  0.30196078]\n",
      "   [ 0.03921569  0.0745098   0.15294118]\n",
      "   ..., \n",
      "   [ 0.07843137  0.11764706  0.23921569]\n",
      "   [ 0.05098039  0.07843137  0.16862745]\n",
      "   [ 0.02352941  0.03921569  0.09019608]]]\n",
      "\n",
      "\n",
      " [[[ 0.7254902   0.79215686  0.81176471]\n",
      "   [ 0.67843137  0.77647059  0.80392157]\n",
      "   [ 0.69019608  0.81176471  0.85098039]\n",
      "   ..., \n",
      "   [ 0.14509804  0.17647059  0.24313725]\n",
      "   [ 0.10196078  0.1372549   0.2       ]\n",
      "   [ 0.12941176  0.19607843  0.27058824]]\n",
      "\n",
      "  [[ 0.42745098  0.47058824  0.46666667]\n",
      "   [ 0.4627451   0.52941176  0.52941176]\n",
      "   [ 0.4745098   0.56078431  0.56470588]\n",
      "   ..., \n",
      "   [ 0.16862745  0.19607843  0.23529412]\n",
      "   [ 0.12941176  0.13333333  0.16862745]\n",
      "   [ 0.15686275  0.18823529  0.22745098]]\n",
      "\n",
      "  [[ 0.20392157  0.22745098  0.21568627]\n",
      "   [ 0.22745098  0.26666667  0.25098039]\n",
      "   [ 0.22745098  0.28235294  0.2627451 ]\n",
      "   ..., \n",
      "   [ 0.18039216  0.22745098  0.26666667]\n",
      "   [ 0.15686275  0.18431373  0.22745098]\n",
      "   [ 0.18431373  0.22745098  0.26666667]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.63529412  0.62745098  0.6745098 ]\n",
      "   [ 0.61960784  0.61176471  0.65490196]\n",
      "   [ 0.62352941  0.61568627  0.65882353]\n",
      "   ..., \n",
      "   [ 0.05882353  0.05098039  0.09019608]\n",
      "   [ 0.05490196  0.04705882  0.09019608]\n",
      "   [ 0.07058824  0.0627451   0.10588235]]\n",
      "\n",
      "  [[ 0.61176471  0.60392157  0.65882353]\n",
      "   [ 0.59607843  0.58823529  0.63921569]\n",
      "   [ 0.59607843  0.58823529  0.63921569]\n",
      "   ..., \n",
      "   [ 0.05098039  0.04313725  0.08235294]\n",
      "   [ 0.05882353  0.05098039  0.09411765]\n",
      "   [ 0.1254902   0.11764706  0.16078431]]\n",
      "\n",
      "  [[ 0.58431373  0.57647059  0.63137255]\n",
      "   [ 0.56470588  0.55686275  0.61176471]\n",
      "   [ 0.57254902  0.56470588  0.61960784]\n",
      "   ..., \n",
      "   [ 0.0627451   0.05490196  0.09411765]\n",
      "   [ 0.20392157  0.19607843  0.23921569]\n",
      "   [ 0.38431373  0.37647059  0.41960784]]]\n",
      "\n",
      "\n",
      " [[[ 0.76470588  0.71764706  0.67058824]\n",
      "   [ 0.75686275  0.70980392  0.6627451 ]\n",
      "   [ 0.76078431  0.71372549  0.66666667]\n",
      "   ..., \n",
      "   [ 0.22352941  0.22352941  0.22352941]\n",
      "   [ 0.20392157  0.20392157  0.20392157]\n",
      "   [ 0.03137255  0.03137255  0.03137255]]\n",
      "\n",
      "  [[ 0.77254902  0.72156863  0.67843137]\n",
      "   [ 0.76470588  0.71764706  0.6745098 ]\n",
      "   [ 0.77254902  0.7254902   0.68235294]\n",
      "   ..., \n",
      "   [ 0.34117647  0.34117647  0.34117647]\n",
      "   [ 0.31372549  0.31372549  0.31372549]\n",
      "   [ 0.03921569  0.03921569  0.03921569]]\n",
      "\n",
      "  [[ 0.77254902  0.72156863  0.68627451]\n",
      "   [ 0.76862745  0.71764706  0.68235294]\n",
      "   [ 0.77647059  0.7254902   0.69411765]\n",
      "   ..., \n",
      "   [ 0.43137255  0.43137255  0.43137255]\n",
      "   [ 0.41568627  0.41568627  0.41568627]\n",
      "   [ 0.04705882  0.04705882  0.04705882]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.78039216  0.7254902   0.69411765]\n",
      "   [ 0.76862745  0.72156863  0.68627451]\n",
      "   [ 0.78039216  0.74117647  0.69803922]\n",
      "   ..., \n",
      "   [ 0.13333333  0.10980392  0.08235294]\n",
      "   [ 0.10980392  0.09019608  0.07058824]\n",
      "   [ 0.08627451  0.0745098   0.0627451 ]]\n",
      "\n",
      "  [[ 0.77254902  0.7254902   0.69019608]\n",
      "   [ 0.76470588  0.71764706  0.68235294]\n",
      "   [ 0.77254902  0.72941176  0.69411765]\n",
      "   ..., \n",
      "   [ 0.15294118  0.12156863  0.08235294]\n",
      "   [ 0.14509804  0.12156863  0.08627451]\n",
      "   [ 0.1372549   0.11372549  0.08627451]]\n",
      "\n",
      "  [[ 0.75686275  0.71764706  0.68235294]\n",
      "   [ 0.75686275  0.70588235  0.6745098 ]\n",
      "   [ 0.76470588  0.70980392  0.67843137]\n",
      "   ..., \n",
      "   [ 0.16470588  0.12941176  0.08235294]\n",
      "   [ 0.16862745  0.12941176  0.09019608]\n",
      "   [ 0.16078431  0.1254902   0.09411765]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 1.          1.          0.99607843]\n",
      "   [ 0.98823529  0.98823529  0.98823529]\n",
      "   [ 0.99215686  0.98823529  0.99607843]\n",
      "   ..., \n",
      "   [ 0.64705882  0.69411765  0.72156863]\n",
      "   [ 0.95294118  0.96470588  0.96862745]\n",
      "   [ 0.99607843  0.99215686  0.98823529]]\n",
      "\n",
      "  [[ 1.          1.          0.99607843]\n",
      "   [ 0.98823529  0.98823529  0.98823529]\n",
      "   [ 0.99607843  0.99607843  1.        ]\n",
      "   ..., \n",
      "   [ 0.50980392  0.56470588  0.63137255]\n",
      "   [ 0.88235294  0.90980392  0.9372549 ]\n",
      "   [ 0.99215686  1.          1.        ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.99607843  0.99607843  0.99607843]\n",
      "   [ 0.97254902  0.96862745  0.97647059]\n",
      "   ..., \n",
      "   [ 0.55294118  0.60784314  0.68627451]\n",
      "   [ 0.8627451   0.89019608  0.92156863]\n",
      "   [ 0.99215686  1.          1.        ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.91372549  0.91764706  0.91764706]\n",
      "   [ 0.84705882  0.84705882  0.84705882]\n",
      "   [ 0.94509804  0.94509804  0.94509804]\n",
      "   ..., \n",
      "   [ 0.03529412  0.04313725  0.04313725]\n",
      "   [ 0.07058824  0.0745098   0.0745098 ]\n",
      "   [ 0.6627451   0.67058824  0.66666667]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 1.          1.          1.        ]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.08235294  0.09019608  0.08627451]\n",
      "   [ 0.44313725  0.45098039  0.44705882]\n",
      "   [ 0.92156863  0.92941176  0.9254902 ]]\n",
      "\n",
      "  [[ 1.          1.          1.        ]\n",
      "   [ 0.98431373  0.98431373  0.98431373]\n",
      "   [ 0.99215686  0.99215686  0.99215686]\n",
      "   ..., \n",
      "   [ 0.6745098   0.68235294  0.67843137]\n",
      "   [ 0.90196078  0.90980392  0.90588235]\n",
      "   [ 0.96862745  0.97254902  0.97254902]]]\n",
      "\n",
      "\n",
      " [[[ 0.49803922  0.56862745  0.65490196]\n",
      "   [ 0.49411765  0.56470588  0.65098039]\n",
      "   [ 0.49803922  0.56862745  0.65490196]\n",
      "   ..., \n",
      "   [ 0.49019608  0.55686275  0.62352941]\n",
      "   [ 0.49019608  0.55686275  0.62352941]\n",
      "   [ 0.48627451  0.55294118  0.61960784]]\n",
      "\n",
      "  [[ 0.49019608  0.56078431  0.64313725]\n",
      "   [ 0.49019608  0.56078431  0.63921569]\n",
      "   [ 0.49411765  0.56470588  0.64313725]\n",
      "   ..., \n",
      "   [ 0.49019608  0.55686275  0.61568627]\n",
      "   [ 0.48627451  0.55686275  0.61176471]\n",
      "   [ 0.48627451  0.55294118  0.61176471]]\n",
      "\n",
      "  [[ 0.49411765  0.56862745  0.63529412]\n",
      "   [ 0.48627451  0.56078431  0.62745098]\n",
      "   [ 0.49411765  0.56862745  0.63529412]\n",
      "   ..., \n",
      "   [ 0.48627451  0.56078431  0.61176471]\n",
      "   [ 0.48235294  0.55294118  0.60784314]\n",
      "   [ 0.48235294  0.55294118  0.60784314]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.32941176  0.40784314  0.4627451 ]\n",
      "   [ 0.33333333  0.40784314  0.4627451 ]\n",
      "   [ 0.34117647  0.41568627  0.47058824]\n",
      "   ..., \n",
      "   [ 0.25490196  0.3254902   0.37254902]\n",
      "   [ 0.30980392  0.38039216  0.42745098]\n",
      "   [ 0.34509804  0.41568627  0.4627451 ]]\n",
      "\n",
      "  [[ 0.3372549   0.41176471  0.47058824]\n",
      "   [ 0.3254902   0.4         0.45490196]\n",
      "   [ 0.3254902   0.4         0.45490196]\n",
      "   ..., \n",
      "   [ 0.28627451  0.35686275  0.40392157]\n",
      "   [ 0.3254902   0.39607843  0.44313725]\n",
      "   [ 0.34117647  0.41176471  0.45882353]]\n",
      "\n",
      "  [[ 0.33333333  0.40784314  0.4627451 ]\n",
      "   [ 0.33333333  0.40392157  0.45882353]\n",
      "   [ 0.3254902   0.4         0.45490196]\n",
      "   ..., \n",
      "   [ 0.28235294  0.35294118  0.4       ]\n",
      "   [ 0.30588235  0.37647059  0.42352941]\n",
      "   [ 0.32156863  0.39215686  0.43921569]]]\n",
      "\n",
      "\n",
      " [[[ 0.45490196  0.27843137  0.10196078]\n",
      "   [ 0.25098039  0.13333333  0.03921569]\n",
      "   [ 0.0745098   0.02352941  0.00784314]\n",
      "   ..., \n",
      "   [ 0.58039216  0.32941176  0.14901961]\n",
      "   [ 0.6627451   0.37647059  0.18039216]\n",
      "   [ 0.7372549   0.44705882  0.23137255]]\n",
      "\n",
      "  [[ 0.44705882  0.26666667  0.08627451]\n",
      "   [ 0.25098039  0.1372549   0.04313725]\n",
      "   [ 0.07058824  0.02352941  0.00784314]\n",
      "   ..., \n",
      "   [ 0.58431373  0.32941176  0.16862745]\n",
      "   [ 0.63921569  0.36862745  0.17647059]\n",
      "   [ 0.7254902   0.44705882  0.23137255]]\n",
      "\n",
      "  [[ 0.44705882  0.25882353  0.09019608]\n",
      "   [ 0.24313725  0.13333333  0.04313725]\n",
      "   [ 0.06666667  0.02352941  0.00784314]\n",
      "   ..., \n",
      "   [ 0.61568627  0.35294118  0.18431373]\n",
      "   [ 0.68627451  0.4         0.2       ]\n",
      "   [ 0.7254902   0.44705882  0.22745098]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.94509804  0.94117647  0.91764706]\n",
      "   [ 0.95294118  0.94901961  0.9254902 ]\n",
      "   [ 0.94509804  0.94509804  0.9254902 ]\n",
      "   ..., \n",
      "   [ 0.12156863  0.07058824  0.01568627]\n",
      "   [ 0.1372549   0.07843137  0.01960784]\n",
      "   [ 0.15294118  0.07843137  0.01960784]]\n",
      "\n",
      "  [[ 0.84313725  0.82745098  0.78823529]\n",
      "   [ 0.90196078  0.89019608  0.8627451 ]\n",
      "   [ 0.92941176  0.92156863  0.90196078]\n",
      "   ..., \n",
      "   [ 0.09019608  0.05098039  0.01176471]\n",
      "   [ 0.09803922  0.05098039  0.00784314]\n",
      "   [ 0.10196078  0.05098039  0.01176471]]\n",
      "\n",
      "  [[ 0.47058824  0.42352941  0.37254902]\n",
      "   [ 0.54117647  0.49803922  0.45098039]\n",
      "   [ 0.60784314  0.57254902  0.5254902 ]\n",
      "   ..., \n",
      "   [ 0.17254902  0.09803922  0.02745098]\n",
      "   [ 0.16078431  0.08627451  0.02352941]\n",
      "   [ 0.14901961  0.0745098   0.01960784]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.23921569  0.28627451  0.29803922]\n",
      "   [ 0.18039216  0.22745098  0.26666667]\n",
      "   [ 0.15294118  0.19215686  0.25882353]\n",
      "   ..., \n",
      "   [ 0.27843137  0.35294118  0.31372549]\n",
      "   [ 0.24313725  0.30588235  0.29411765]\n",
      "   [ 0.17647059  0.22745098  0.23921569]]\n",
      "\n",
      "  [[ 0.24705882  0.29411765  0.30196078]\n",
      "   [ 0.17647059  0.22745098  0.2627451 ]\n",
      "   [ 0.1254902   0.16862745  0.22745098]\n",
      "   ..., \n",
      "   [ 0.28627451  0.34901961  0.32156863]\n",
      "   [ 0.27843137  0.32941176  0.31372549]\n",
      "   [ 0.19607843  0.23529412  0.24313725]]\n",
      "\n",
      "  [[ 0.24705882  0.31372549  0.30196078]\n",
      "   [ 0.22352941  0.29411765  0.29411765]\n",
      "   [ 0.24705882  0.30980392  0.31764706]\n",
      "   ..., \n",
      "   [ 0.32156863  0.37647059  0.35686275]\n",
      "   [ 0.29803922  0.34509804  0.32156863]\n",
      "   [ 0.20784314  0.24705882  0.24313725]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.51372549  0.52156863  0.4       ]\n",
      "   [ 0.61176471  0.60392157  0.4627451 ]\n",
      "   [ 0.63137255  0.61568627  0.4745098 ]\n",
      "   ..., \n",
      "   [ 0.91764706  0.8745098   0.67843137]\n",
      "   [ 0.8627451   0.8         0.61176471]\n",
      "   [ 0.70980392  0.65098039  0.49411765]]\n",
      "\n",
      "  [[ 0.41176471  0.42352941  0.3254902 ]\n",
      "   [ 0.41960784  0.42352941  0.30980392]\n",
      "   [ 0.45098039  0.45098039  0.32941176]\n",
      "   ..., \n",
      "   [ 0.92156863  0.86666667  0.6745098 ]\n",
      "   [ 0.89803922  0.81568627  0.63137255]\n",
      "   [ 0.74901961  0.66666667  0.51372549]]\n",
      "\n",
      "  [[ 0.2627451   0.29019608  0.23921569]\n",
      "   [ 0.30588235  0.3254902   0.26666667]\n",
      "   [ 0.43921569  0.45098039  0.35294118]\n",
      "   ..., \n",
      "   [ 0.89019608  0.80784314  0.58823529]\n",
      "   [ 0.85098039  0.75294118  0.54509804]\n",
      "   [ 0.72156863  0.62745098  0.45882353]]]\n",
      "\n",
      "\n",
      " [[[ 0.03921569  0.01568627  0.05490196]\n",
      "   [ 0.04313725  0.02352941  0.05882353]\n",
      "   [ 0.07843137  0.08627451  0.09019608]\n",
      "   ..., \n",
      "   [ 0.23137255  0.27843137  0.21568627]\n",
      "   [ 0.22352941  0.2745098   0.21568627]\n",
      "   [ 0.20784314  0.26666667  0.23137255]]\n",
      "\n",
      "  [[ 0.03921569  0.01568627  0.05490196]\n",
      "   [ 0.04313725  0.03529412  0.05882353]\n",
      "   [ 0.09803922  0.1254902   0.10980392]\n",
      "   ..., \n",
      "   [ 0.21568627  0.23921569  0.18823529]\n",
      "   [ 0.25882353  0.29803922  0.22745098]\n",
      "   [ 0.18823529  0.24705882  0.21176471]]\n",
      "\n",
      "  [[ 0.04705882  0.02352941  0.0627451 ]\n",
      "   [ 0.04313725  0.03921569  0.05882353]\n",
      "   [ 0.14901961  0.18431373  0.14901961]\n",
      "   ..., \n",
      "   [ 0.18431373  0.20392157  0.16470588]\n",
      "   [ 0.22352941  0.25882353  0.19215686]\n",
      "   [ 0.20392157  0.25098039  0.2       ]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.69411765  0.6627451   0.71764706]\n",
      "   [ 0.70588235  0.6627451   0.72156863]\n",
      "   [ 0.72156863  0.6745098   0.74509804]\n",
      "   ..., \n",
      "   [ 0.6745098   0.62352941  0.68235294]\n",
      "   [ 0.66666667  0.61568627  0.67058824]\n",
      "   [ 0.64313725  0.58823529  0.64705882]]\n",
      "\n",
      "  [[ 0.62352941  0.61568627  0.69019608]\n",
      "   [ 0.63529412  0.61568627  0.69411765]\n",
      "   [ 0.65490196  0.63529412  0.71764706]\n",
      "   ..., \n",
      "   [ 0.72156863  0.69803922  0.78431373]\n",
      "   [ 0.70980392  0.68627451  0.77254902]\n",
      "   [ 0.69803922  0.67843137  0.76470588]]\n",
      "\n",
      "  [[ 0.61960784  0.61960784  0.72941176]\n",
      "   [ 0.62352941  0.62352941  0.73333333]\n",
      "   [ 0.63921569  0.63921569  0.74509804]\n",
      "   ..., \n",
      "   [ 0.70980392  0.70588235  0.82745098]\n",
      "   [ 0.70196078  0.69411765  0.81568627]\n",
      "   [ 0.68627451  0.68235294  0.80392157]]]\n",
      "\n",
      "\n",
      " [[[ 0.68627451  0.75686275  0.89803922]\n",
      "   [ 0.6745098   0.75294118  0.9254902 ]\n",
      "   [ 0.67058824  0.75686275  0.94117647]\n",
      "   ..., \n",
      "   [ 0.75686275  0.80784314  0.93333333]\n",
      "   [ 0.76862745  0.80784314  0.90588235]\n",
      "   [ 0.76078431  0.79607843  0.89019608]]\n",
      "\n",
      "  [[ 0.65490196  0.73333333  0.88627451]\n",
      "   [ 0.64313725  0.73333333  0.90196078]\n",
      "   [ 0.64313725  0.7372549   0.90980392]\n",
      "   ..., \n",
      "   [ 0.70196078  0.75686275  0.88235294]\n",
      "   [ 0.69803922  0.74901961  0.85098039]\n",
      "   [ 0.69019608  0.73333333  0.83137255]]\n",
      "\n",
      "  [[ 0.65490196  0.72156863  0.86666667]\n",
      "   [ 0.65490196  0.72941176  0.87058824]\n",
      "   [ 0.67058824  0.72941176  0.85098039]\n",
      "   ..., \n",
      "   [ 0.69019608  0.74901961  0.87843137]\n",
      "   [ 0.68235294  0.74117647  0.85098039]\n",
      "   [ 0.67058824  0.72156863  0.82745098]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.33333333  0.32941176  0.39607843]\n",
      "   [ 0.33333333  0.32156863  0.36470588]\n",
      "   [ 0.36078431  0.32941176  0.32156863]\n",
      "   ..., \n",
      "   [ 0.4745098   0.44313725  0.47058824]\n",
      "   [ 0.41960784  0.40392157  0.46666667]\n",
      "   [ 0.45882353  0.43921569  0.50588235]]\n",
      "\n",
      "  [[ 0.33333333  0.34117647  0.4       ]\n",
      "   [ 0.32941176  0.31764706  0.34901961]\n",
      "   [ 0.34117647  0.30980392  0.30588235]\n",
      "   ..., \n",
      "   [ 0.30196078  0.28235294  0.32941176]\n",
      "   [ 0.43137255  0.40392157  0.47058824]\n",
      "   [ 0.44705882  0.41960784  0.48627451]]\n",
      "\n",
      "  [[ 0.32156863  0.32941176  0.37647059]\n",
      "   [ 0.29411765  0.29019608  0.32156863]\n",
      "   [ 0.22352941  0.19607843  0.21568627]\n",
      "   ..., \n",
      "   [ 0.30196078  0.26666667  0.30588235]\n",
      "   [ 0.35686275  0.30588235  0.35294118]\n",
      "   [ 0.35686275  0.30588235  0.35294118]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.54901961  0.49019608  0.45098039]\n",
      "   [ 0.57254902  0.50980392  0.47843137]\n",
      "   [ 0.56078431  0.49803922  0.47843137]\n",
      "   ..., \n",
      "   [ 0.66666667  0.56862745  0.51372549]\n",
      "   [ 0.69019608  0.58823529  0.5254902 ]\n",
      "   [ 0.66666667  0.57647059  0.52156863]]\n",
      "\n",
      "  [[ 0.4745098   0.42352941  0.50588235]\n",
      "   [ 0.50980392  0.4627451   0.54509804]\n",
      "   [ 0.5254902   0.4745098   0.56078431]\n",
      "   ..., \n",
      "   [ 0.63921569  0.55294118  0.61568627]\n",
      "   [ 0.66666667  0.57254902  0.63137255]\n",
      "   [ 0.66666667  0.58039216  0.63137255]]\n",
      "\n",
      "  [[ 0.59607843  0.54509804  0.68235294]\n",
      "   [ 0.61568627  0.56862745  0.70196078]\n",
      "   [ 0.60784314  0.56078431  0.68627451]\n",
      "   ..., \n",
      "   [ 0.69411765  0.60392157  0.75686275]\n",
      "   [ 0.70980392  0.61176471  0.76078431]\n",
      "   [ 0.71764706  0.62745098  0.76078431]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.49019608  0.43137255  0.4       ]\n",
      "   [ 0.50588235  0.43921569  0.40392157]\n",
      "   [ 0.29803922  0.2627451   0.18431373]\n",
      "   ..., \n",
      "   [ 0.65882353  0.5372549   0.47058824]\n",
      "   [ 0.61960784  0.49411765  0.40392157]\n",
      "   [ 0.57254902  0.45490196  0.34117647]]\n",
      "\n",
      "  [[ 0.33333333  0.30196078  0.2745098 ]\n",
      "   [ 0.36862745  0.31764706  0.27843137]\n",
      "   [ 0.29019608  0.25490196  0.17647059]\n",
      "   ..., \n",
      "   [ 0.63529412  0.51764706  0.41568627]\n",
      "   [ 0.65098039  0.5254902   0.39215686]\n",
      "   [ 0.61960784  0.50196078  0.36078431]]\n",
      "\n",
      "  [[ 0.49019608  0.43921569  0.43529412]\n",
      "   [ 0.50980392  0.44313725  0.43529412]\n",
      "   [ 0.41176471  0.35686275  0.29411765]\n",
      "   ..., \n",
      "   [ 0.51764706  0.41568627  0.30588235]\n",
      "   [ 0.50980392  0.39607843  0.25098039]\n",
      "   [ 0.55686275  0.45098039  0.30588235]]]\n",
      "\n",
      "\n",
      " [[[ 0.39215686  0.42745098  0.32941176]\n",
      "   [ 0.47843137  0.49411765  0.42745098]\n",
      "   [ 0.34117647  0.34117647  0.29803922]\n",
      "   ..., \n",
      "   [ 0.29411765  0.30588235  0.27058824]\n",
      "   [ 0.2745098   0.28627451  0.25098039]\n",
      "   [ 0.2745098   0.28627451  0.25098039]]\n",
      "\n",
      "  [[ 0.3372549   0.38823529  0.27843137]\n",
      "   [ 0.29803922  0.32941176  0.25882353]\n",
      "   [ 0.23529412  0.25098039  0.21176471]\n",
      "   ..., \n",
      "   [ 0.30588235  0.31764706  0.28235294]\n",
      "   [ 0.29803922  0.30980392  0.2745098 ]\n",
      "   [ 0.32156863  0.33333333  0.29803922]]\n",
      "\n",
      "  [[ 0.32941176  0.39215686  0.28627451]\n",
      "   [ 0.3254902   0.37254902  0.29411765]\n",
      "   [ 0.30196078  0.3372549   0.28235294]\n",
      "   ..., \n",
      "   [ 0.29019608  0.30196078  0.26666667]\n",
      "   [ 0.28627451  0.29803922  0.2627451 ]\n",
      "   [ 0.3254902   0.3372549   0.30196078]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.25098039  0.30196078  0.30980392]\n",
      "   [ 0.47843137  0.52156863  0.56470588]\n",
      "   [ 0.5254902   0.56862745  0.61176471]\n",
      "   ..., \n",
      "   [ 0.41176471  0.48235294  0.47058824]\n",
      "   [ 0.32941176  0.40392157  0.35686275]\n",
      "   [ 0.23529412  0.34509804  0.24705882]]\n",
      "\n",
      "  [[ 0.17254902  0.2         0.21960784]\n",
      "   [ 0.30588235  0.32941176  0.36862745]\n",
      "   [ 0.37647059  0.39607843  0.43137255]\n",
      "   ..., \n",
      "   [ 0.57647059  0.64705882  0.69803922]\n",
      "   [ 0.49411765  0.56078431  0.58431373]\n",
      "   [ 0.36862745  0.45882353  0.44313725]]\n",
      "\n",
      "  [[ 0.14117647  0.1372549   0.15294118]\n",
      "   [ 0.23137255  0.22745098  0.25882353]\n",
      "   [ 0.32156863  0.31764706  0.33333333]\n",
      "   ..., \n",
      "   [ 0.5254902   0.6         0.62745098]\n",
      "   [ 0.54117647  0.59607843  0.61960784]\n",
      "   [ 0.50980392  0.58039216  0.58823529]]]\n",
      "\n",
      "\n",
      " [[[ 0.0745098   0.1254902   0.05882353]\n",
      "   [ 0.08235294  0.14901961  0.08235294]\n",
      "   [ 0.10588235  0.19215686  0.1254902 ]\n",
      "   ..., \n",
      "   [ 0.29411765  0.48627451  0.51372549]\n",
      "   [ 0.29803922  0.48627451  0.50980392]\n",
      "   [ 0.27843137  0.4627451   0.48627451]]\n",
      "\n",
      "  [[ 0.09019608  0.12156863  0.05490196]\n",
      "   [ 0.08235294  0.11764706  0.04705882]\n",
      "   [ 0.09019608  0.1372549   0.05490196]\n",
      "   ..., \n",
      "   [ 0.28235294  0.4627451   0.49411765]\n",
      "   [ 0.29411765  0.46666667  0.48627451]\n",
      "   [ 0.26666667  0.43529412  0.44705882]]\n",
      "\n",
      "  [[ 0.09411765  0.14509804  0.06666667]\n",
      "   [ 0.08627451  0.1372549   0.0627451 ]\n",
      "   [ 0.09411765  0.14117647  0.07058824]\n",
      "   ..., \n",
      "   [ 0.25098039  0.42745098  0.43921569]\n",
      "   [ 0.2627451   0.42745098  0.43529412]\n",
      "   [ 0.25098039  0.41176471  0.41176471]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.24313725  0.18039216  0.09019608]\n",
      "   [ 0.23529412  0.18039216  0.10588235]\n",
      "   [ 0.21568627  0.18823529  0.10980392]\n",
      "   ..., \n",
      "   [ 0.05098039  0.02352941  0.01568627]\n",
      "   [ 0.04705882  0.05490196  0.03137255]\n",
      "   [ 0.09803922  0.15686275  0.11764706]]\n",
      "\n",
      "  [[ 0.24705882  0.20784314  0.11764706]\n",
      "   [ 0.19215686  0.17647059  0.08627451]\n",
      "   [ 0.17647059  0.18039216  0.09019608]\n",
      "   ..., \n",
      "   [ 0.11372549  0.1372549   0.12156863]\n",
      "   [ 0.11764706  0.16470588  0.14509804]\n",
      "   [ 0.10588235  0.19607843  0.16862745]]\n",
      "\n",
      "  [[ 0.27058824  0.20392157  0.11372549]\n",
      "   [ 0.19215686  0.14901961  0.07843137]\n",
      "   [ 0.21176471  0.18039216  0.10588235]\n",
      "   ..., \n",
      "   [ 0.25882353  0.34509804  0.33333333]\n",
      "   [ 0.15686275  0.26666667  0.25098039]\n",
      "   [ 0.11372549  0.24313725  0.22745098]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.1372549   0.69803922  0.92156863]\n",
      "   [ 0.15686275  0.69019608  0.9372549 ]\n",
      "   [ 0.16470588  0.69019608  0.94509804]\n",
      "   ..., \n",
      "   [ 0.38823529  0.69411765  0.85882353]\n",
      "   [ 0.30980392  0.57647059  0.77254902]\n",
      "   [ 0.34901961  0.58039216  0.74117647]]\n",
      "\n",
      "  [[ 0.22352941  0.71372549  0.91764706]\n",
      "   [ 0.17254902  0.72156863  0.98039216]\n",
      "   [ 0.19607843  0.71764706  0.94117647]\n",
      "   ..., \n",
      "   [ 0.61176471  0.71372549  0.78431373]\n",
      "   [ 0.55294118  0.69411765  0.80784314]\n",
      "   [ 0.45490196  0.58431373  0.68627451]]\n",
      "\n",
      "  [[ 0.38431373  0.77254902  0.92941176]\n",
      "   [ 0.25098039  0.74117647  0.98823529]\n",
      "   [ 0.27058824  0.75294118  0.96078431]\n",
      "   ..., \n",
      "   [ 0.7372549   0.76470588  0.80784314]\n",
      "   [ 0.46666667  0.52941176  0.57647059]\n",
      "   [ 0.23921569  0.30980392  0.35294118]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.28627451  0.30980392  0.30196078]\n",
      "   [ 0.20784314  0.24705882  0.26666667]\n",
      "   [ 0.21176471  0.26666667  0.31372549]\n",
      "   ..., \n",
      "   [ 0.06666667  0.15686275  0.25098039]\n",
      "   [ 0.08235294  0.14117647  0.2       ]\n",
      "   [ 0.12941176  0.18823529  0.19215686]]\n",
      "\n",
      "  [[ 0.23921569  0.26666667  0.29411765]\n",
      "   [ 0.21568627  0.2745098   0.3372549 ]\n",
      "   [ 0.22352941  0.30980392  0.40392157]\n",
      "   ..., \n",
      "   [ 0.09411765  0.18823529  0.28235294]\n",
      "   [ 0.06666667  0.1372549   0.20784314]\n",
      "   [ 0.02745098  0.09019608  0.1254902 ]]\n",
      "\n",
      "  [[ 0.17254902  0.21960784  0.28627451]\n",
      "   [ 0.18039216  0.25882353  0.34509804]\n",
      "   [ 0.19215686  0.30196078  0.41176471]\n",
      "   ..., \n",
      "   [ 0.10588235  0.20392157  0.30196078]\n",
      "   [ 0.08235294  0.16862745  0.25882353]\n",
      "   [ 0.04705882  0.12156863  0.19607843]]]\n",
      "\n",
      "\n",
      " [[[ 0.74117647  0.82745098  0.94117647]\n",
      "   [ 0.72941176  0.81568627  0.9254902 ]\n",
      "   [ 0.7254902   0.81176471  0.92156863]\n",
      "   ..., \n",
      "   [ 0.68627451  0.76470588  0.87843137]\n",
      "   [ 0.6745098   0.76078431  0.87058824]\n",
      "   [ 0.6627451   0.76078431  0.8627451 ]]\n",
      "\n",
      "  [[ 0.76078431  0.82352941  0.9372549 ]\n",
      "   [ 0.74901961  0.81176471  0.9254902 ]\n",
      "   [ 0.74509804  0.80784314  0.92156863]\n",
      "   ..., \n",
      "   [ 0.67843137  0.75294118  0.8627451 ]\n",
      "   [ 0.67058824  0.74901961  0.85490196]\n",
      "   [ 0.65490196  0.74509804  0.84705882]]\n",
      "\n",
      "  [[ 0.81568627  0.85882353  0.95686275]\n",
      "   [ 0.80392157  0.84705882  0.94117647]\n",
      "   [ 0.8         0.84313725  0.9372549 ]\n",
      "   ..., \n",
      "   [ 0.68627451  0.74901961  0.85098039]\n",
      "   [ 0.6745098   0.74509804  0.84705882]\n",
      "   [ 0.6627451   0.74901961  0.84313725]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.81176471  0.78039216  0.70980392]\n",
      "   [ 0.79607843  0.76470588  0.68627451]\n",
      "   [ 0.79607843  0.76862745  0.67843137]\n",
      "   ..., \n",
      "   [ 0.52941176  0.51764706  0.49803922]\n",
      "   [ 0.63529412  0.61960784  0.58823529]\n",
      "   [ 0.65882353  0.63921569  0.59215686]]\n",
      "\n",
      "  [[ 0.77647059  0.74509804  0.66666667]\n",
      "   [ 0.74117647  0.70980392  0.62352941]\n",
      "   [ 0.70588235  0.6745098   0.57647059]\n",
      "   ..., \n",
      "   [ 0.69803922  0.67058824  0.62745098]\n",
      "   [ 0.68627451  0.6627451   0.61176471]\n",
      "   [ 0.68627451  0.6627451   0.60392157]]\n",
      "\n",
      "  [[ 0.77647059  0.74117647  0.67843137]\n",
      "   [ 0.74117647  0.70980392  0.63529412]\n",
      "   [ 0.69803922  0.66666667  0.58431373]\n",
      "   ..., \n",
      "   [ 0.76470588  0.72156863  0.6627451 ]\n",
      "   [ 0.76862745  0.74117647  0.67058824]\n",
      "   [ 0.76470588  0.74509804  0.67058824]]]\n",
      "\n",
      "\n",
      " [[[ 0.89803922  0.89803922  0.9372549 ]\n",
      "   [ 0.9254902   0.92941176  0.96862745]\n",
      "   [ 0.91764706  0.9254902   0.96862745]\n",
      "   ..., \n",
      "   [ 0.85098039  0.85882353  0.91372549]\n",
      "   [ 0.86666667  0.8745098   0.91764706]\n",
      "   [ 0.87058824  0.8745098   0.91372549]]\n",
      "\n",
      "  [[ 0.87058824  0.86666667  0.89803922]\n",
      "   [ 0.9372549   0.9372549   0.97647059]\n",
      "   [ 0.91372549  0.91764706  0.96470588]\n",
      "   ..., \n",
      "   [ 0.8745098   0.8745098   0.9254902 ]\n",
      "   [ 0.89019608  0.89411765  0.93333333]\n",
      "   [ 0.82352941  0.82745098  0.8627451 ]]\n",
      "\n",
      "  [[ 0.83529412  0.80784314  0.82745098]\n",
      "   [ 0.91764706  0.90980392  0.9372549 ]\n",
      "   [ 0.90588235  0.91372549  0.95686275]\n",
      "   ..., \n",
      "   [ 0.8627451   0.8627451   0.90980392]\n",
      "   [ 0.8627451   0.85882353  0.90980392]\n",
      "   [ 0.79215686  0.79607843  0.84313725]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.58823529  0.56078431  0.52941176]\n",
      "   [ 0.54901961  0.52941176  0.49803922]\n",
      "   [ 0.51764706  0.49803922  0.47058824]\n",
      "   ..., \n",
      "   [ 0.87843137  0.87058824  0.85490196]\n",
      "   [ 0.90196078  0.89411765  0.88235294]\n",
      "   [ 0.94509804  0.94509804  0.93333333]]\n",
      "\n",
      "  [[ 0.5372549   0.51764706  0.49411765]\n",
      "   [ 0.50980392  0.49803922  0.47058824]\n",
      "   [ 0.49019608  0.4745098   0.45098039]\n",
      "   ..., \n",
      "   [ 0.70980392  0.70588235  0.69803922]\n",
      "   [ 0.79215686  0.78823529  0.77647059]\n",
      "   [ 0.83137255  0.82745098  0.81176471]]\n",
      "\n",
      "  [[ 0.47843137  0.46666667  0.44705882]\n",
      "   [ 0.4627451   0.45490196  0.43137255]\n",
      "   [ 0.47058824  0.45490196  0.43529412]\n",
      "   ..., \n",
      "   [ 0.70196078  0.69411765  0.67843137]\n",
      "   [ 0.64313725  0.64313725  0.63529412]\n",
      "   [ 0.63921569  0.63921569  0.63137255]]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0.61960784  0.43921569  0.19215686]\n",
      "   [ 0.62352941  0.43529412  0.18431373]\n",
      "   [ 0.64705882  0.45490196  0.2       ]\n",
      "   ..., \n",
      "   [ 0.5372549   0.37254902  0.14117647]\n",
      "   [ 0.49411765  0.35686275  0.14117647]\n",
      "   [ 0.45490196  0.33333333  0.12941176]]\n",
      "\n",
      "  [[ 0.59607843  0.43921569  0.2       ]\n",
      "   [ 0.59215686  0.43137255  0.15686275]\n",
      "   [ 0.62352941  0.44705882  0.17647059]\n",
      "   ..., \n",
      "   [ 0.53333333  0.37254902  0.12156863]\n",
      "   [ 0.49019608  0.35686275  0.1254902 ]\n",
      "   [ 0.46666667  0.34509804  0.13333333]]\n",
      "\n",
      "  [[ 0.59215686  0.43137255  0.18431373]\n",
      "   [ 0.59215686  0.42745098  0.12941176]\n",
      "   [ 0.61960784  0.43529412  0.14117647]\n",
      "   ..., \n",
      "   [ 0.54509804  0.38431373  0.13333333]\n",
      "   [ 0.50980392  0.37254902  0.13333333]\n",
      "   [ 0.47058824  0.34901961  0.12941176]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.26666667  0.48627451  0.69411765]\n",
      "   [ 0.16470588  0.39215686  0.58039216]\n",
      "   [ 0.12156863  0.34509804  0.5372549 ]\n",
      "   ..., \n",
      "   [ 0.14901961  0.38039216  0.57254902]\n",
      "   [ 0.05098039  0.25098039  0.42352941]\n",
      "   [ 0.15686275  0.33333333  0.49803922]]\n",
      "\n",
      "  [[ 0.23921569  0.45490196  0.65882353]\n",
      "   [ 0.19215686  0.4         0.58039216]\n",
      "   [ 0.1372549   0.33333333  0.51764706]\n",
      "   ..., \n",
      "   [ 0.10196078  0.32156863  0.50980392]\n",
      "   [ 0.11372549  0.32156863  0.49411765]\n",
      "   [ 0.07843137  0.25098039  0.41960784]]\n",
      "\n",
      "  [[ 0.21176471  0.41960784  0.62745098]\n",
      "   [ 0.21960784  0.41176471  0.58431373]\n",
      "   [ 0.17647059  0.34901961  0.51764706]\n",
      "   ..., \n",
      "   [ 0.09411765  0.30196078  0.48627451]\n",
      "   [ 0.13333333  0.32941176  0.50588235]\n",
      "   [ 0.08235294  0.2627451   0.43137255]]]\n",
      "\n",
      "\n",
      " [[[ 0.92156863  0.92156863  0.92156863]\n",
      "   [ 0.90588235  0.90588235  0.90588235]\n",
      "   [ 0.90980392  0.90980392  0.90980392]\n",
      "   ..., \n",
      "   [ 0.91372549  0.91372549  0.91372549]\n",
      "   [ 0.91372549  0.91372549  0.91372549]\n",
      "   [ 0.90980392  0.90980392  0.90980392]]\n",
      "\n",
      "  [[ 0.93333333  0.93333333  0.93333333]\n",
      "   [ 0.92156863  0.92156863  0.92156863]\n",
      "   [ 0.92156863  0.92156863  0.92156863]\n",
      "   ..., \n",
      "   [ 0.9254902   0.9254902   0.9254902 ]\n",
      "   [ 0.9254902   0.9254902   0.9254902 ]\n",
      "   [ 0.92156863  0.92156863  0.92156863]]\n",
      "\n",
      "  [[ 0.92941176  0.92941176  0.92941176]\n",
      "   [ 0.91764706  0.91764706  0.91764706]\n",
      "   [ 0.91764706  0.91764706  0.91764706]\n",
      "   ..., \n",
      "   [ 0.92156863  0.92156863  0.92156863]\n",
      "   [ 0.92156863  0.92156863  0.92156863]\n",
      "   [ 0.91764706  0.91764706  0.91764706]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.34117647  0.38823529  0.34901961]\n",
      "   [ 0.16862745  0.2         0.14509804]\n",
      "   [ 0.0745098   0.09019608  0.04313725]\n",
      "   ..., \n",
      "   [ 0.6627451   0.72156863  0.70196078]\n",
      "   [ 0.71372549  0.77254902  0.75686275]\n",
      "   [ 0.7372549   0.79215686  0.78823529]]\n",
      "\n",
      "  [[ 0.32156863  0.37647059  0.32156863]\n",
      "   [ 0.18039216  0.22352941  0.14117647]\n",
      "   [ 0.14117647  0.17254902  0.08627451]\n",
      "   ..., \n",
      "   [ 0.68235294  0.74117647  0.71764706]\n",
      "   [ 0.7254902   0.78431373  0.76862745]\n",
      "   [ 0.73333333  0.79215686  0.78431373]]\n",
      "\n",
      "  [[ 0.33333333  0.39607843  0.3254902 ]\n",
      "   [ 0.24313725  0.29411765  0.18823529]\n",
      "   [ 0.22745098  0.2627451   0.14901961]\n",
      "   ..., \n",
      "   [ 0.65882353  0.71764706  0.69803922]\n",
      "   [ 0.70588235  0.76470588  0.74901961]\n",
      "   [ 0.72941176  0.78431373  0.78039216]]]\n",
      "\n",
      "\n",
      " [[[ 0.61960784  0.74509804  0.87058824]\n",
      "   [ 0.61960784  0.73333333  0.85490196]\n",
      "   [ 0.54509804  0.65098039  0.76078431]\n",
      "   ..., \n",
      "   [ 0.89411765  0.90588235  0.91764706]\n",
      "   [ 0.92941176  0.9372549   0.95294118]\n",
      "   [ 0.93333333  0.94509804  0.96470588]]\n",
      "\n",
      "  [[ 0.66666667  0.78431373  0.89803922]\n",
      "   [ 0.6745098   0.78039216  0.88627451]\n",
      "   [ 0.59215686  0.69019608  0.78823529]\n",
      "   ..., \n",
      "   [ 0.90980392  0.90980392  0.9254902 ]\n",
      "   [ 0.96470588  0.96470588  0.98039216]\n",
      "   [ 0.96470588  0.96862745  0.98431373]]\n",
      "\n",
      "  [[ 0.68235294  0.78823529  0.88235294]\n",
      "   [ 0.69019608  0.78431373  0.87058824]\n",
      "   [ 0.61568627  0.70196078  0.78039216]\n",
      "   ..., \n",
      "   [ 0.90196078  0.89803922  0.90980392]\n",
      "   [ 0.98039216  0.97647059  0.98431373]\n",
      "   [ 0.96078431  0.95686275  0.96862745]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.12156863  0.15686275  0.17647059]\n",
      "   [ 0.11764706  0.15294118  0.17254902]\n",
      "   [ 0.10196078  0.1372549   0.15686275]\n",
      "   ..., \n",
      "   [ 0.14509804  0.15686275  0.18039216]\n",
      "   [ 0.03529412  0.05098039  0.05490196]\n",
      "   [ 0.01568627  0.02745098  0.01960784]]\n",
      "\n",
      "  [[ 0.09019608  0.13333333  0.15294118]\n",
      "   [ 0.10588235  0.14901961  0.16862745]\n",
      "   [ 0.09803922  0.14117647  0.16078431]\n",
      "   ..., \n",
      "   [ 0.0745098   0.07843137  0.09411765]\n",
      "   [ 0.01568627  0.02352941  0.01176471]\n",
      "   [ 0.01960784  0.02745098  0.01176471]]\n",
      "\n",
      "  [[ 0.10980392  0.16078431  0.18431373]\n",
      "   [ 0.11764706  0.16862745  0.19607843]\n",
      "   [ 0.1254902   0.17647059  0.20392157]\n",
      "   ..., \n",
      "   [ 0.01960784  0.02352941  0.03137255]\n",
      "   [ 0.01568627  0.01960784  0.01176471]\n",
      "   [ 0.02745098  0.03137255  0.02745098]]]\n",
      "\n",
      "\n",
      " ..., \n",
      " [[[ 0.07843137  0.05882353  0.04705882]\n",
      "   [ 0.0745098   0.05490196  0.04313725]\n",
      "   [ 0.05882353  0.05490196  0.04313725]\n",
      "   ..., \n",
      "   [ 0.03921569  0.03529412  0.02745098]\n",
      "   [ 0.04705882  0.04313725  0.03529412]\n",
      "   [ 0.05098039  0.04705882  0.03921569]]\n",
      "\n",
      "  [[ 0.08235294  0.0627451   0.05098039]\n",
      "   [ 0.07843137  0.0627451   0.05098039]\n",
      "   [ 0.07058824  0.06666667  0.04705882]\n",
      "   ..., \n",
      "   [ 0.03921569  0.03529412  0.02745098]\n",
      "   [ 0.03921569  0.03529412  0.02745098]\n",
      "   [ 0.04705882  0.04313725  0.03529412]]\n",
      "\n",
      "  [[ 0.08235294  0.0627451   0.05098039]\n",
      "   [ 0.08235294  0.06666667  0.04705882]\n",
      "   [ 0.07843137  0.07058824  0.04313725]\n",
      "   ..., \n",
      "   [ 0.04705882  0.04313725  0.03529412]\n",
      "   [ 0.04705882  0.04313725  0.03529412]\n",
      "   [ 0.05098039  0.04705882  0.03921569]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.12941176  0.09803922  0.05098039]\n",
      "   [ 0.13333333  0.10196078  0.05882353]\n",
      "   [ 0.13333333  0.10196078  0.05882353]\n",
      "   ..., \n",
      "   [ 0.10980392  0.09803922  0.20392157]\n",
      "   [ 0.11372549  0.09803922  0.22745098]\n",
      "   [ 0.09019608  0.07843137  0.16470588]]\n",
      "\n",
      "  [[ 0.12941176  0.09803922  0.05490196]\n",
      "   [ 0.13333333  0.10196078  0.05882353]\n",
      "   [ 0.13333333  0.10196078  0.05882353]\n",
      "   ..., \n",
      "   [ 0.10588235  0.09411765  0.20392157]\n",
      "   [ 0.10588235  0.09411765  0.21960784]\n",
      "   [ 0.09803922  0.08627451  0.18431373]]\n",
      "\n",
      "  [[ 0.12156863  0.09019608  0.04705882]\n",
      "   [ 0.1254902   0.09411765  0.05098039]\n",
      "   [ 0.12941176  0.09803922  0.05490196]\n",
      "   ..., \n",
      "   [ 0.09411765  0.09019608  0.19607843]\n",
      "   [ 0.10196078  0.09019608  0.20784314]\n",
      "   [ 0.09803922  0.07843137  0.18431373]]]\n",
      "\n",
      "\n",
      " [[[ 0.09803922  0.15686275  0.04705882]\n",
      "   [ 0.05882353  0.14117647  0.01176471]\n",
      "   [ 0.09019608  0.16078431  0.07058824]\n",
      "   ..., \n",
      "   [ 0.23921569  0.32156863  0.30588235]\n",
      "   [ 0.36078431  0.44313725  0.43921569]\n",
      "   [ 0.29411765  0.34901961  0.36078431]]\n",
      "\n",
      "  [[ 0.04705882  0.09803922  0.02352941]\n",
      "   [ 0.07843137  0.14509804  0.02745098]\n",
      "   [ 0.09411765  0.14117647  0.05882353]\n",
      "   ..., \n",
      "   [ 0.45098039  0.5254902   0.54117647]\n",
      "   [ 0.58431373  0.65882353  0.69411765]\n",
      "   [ 0.40784314  0.45882353  0.51372549]]\n",
      "\n",
      "  [[ 0.04705882  0.09803922  0.04313725]\n",
      "   [ 0.05882353  0.11372549  0.02352941]\n",
      "   [ 0.13333333  0.15686275  0.09411765]\n",
      "   ..., \n",
      "   [ 0.60392157  0.6745098   0.71372549]\n",
      "   [ 0.61568627  0.68627451  0.75294118]\n",
      "   [ 0.45490196  0.50588235  0.59215686]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.39215686  0.50588235  0.31764706]\n",
      "   [ 0.40392157  0.51764706  0.32941176]\n",
      "   [ 0.40784314  0.5254902   0.3372549 ]\n",
      "   ..., \n",
      "   [ 0.38039216  0.50196078  0.32941176]\n",
      "   [ 0.38431373  0.49411765  0.32941176]\n",
      "   [ 0.35686275  0.4745098   0.30980392]]\n",
      "\n",
      "  [[ 0.40392157  0.51764706  0.3254902 ]\n",
      "   [ 0.40784314  0.51372549  0.3254902 ]\n",
      "   [ 0.41960784  0.52941176  0.34117647]\n",
      "   ..., \n",
      "   [ 0.39607843  0.51764706  0.34117647]\n",
      "   [ 0.38823529  0.49803922  0.32941176]\n",
      "   [ 0.36078431  0.4745098   0.30980392]]\n",
      "\n",
      "  [[ 0.37254902  0.49411765  0.30588235]\n",
      "   [ 0.37254902  0.48235294  0.29803922]\n",
      "   [ 0.39607843  0.50196078  0.31764706]\n",
      "   ..., \n",
      "   [ 0.36470588  0.48627451  0.31372549]\n",
      "   [ 0.37254902  0.48235294  0.31764706]\n",
      "   [ 0.36078431  0.47058824  0.31372549]]]\n",
      "\n",
      "\n",
      " [[[ 0.28627451  0.30588235  0.29411765]\n",
      "   [ 0.38431373  0.40392157  0.44313725]\n",
      "   [ 0.38823529  0.41568627  0.44705882]\n",
      "   ..., \n",
      "   [ 0.52941176  0.58823529  0.59607843]\n",
      "   [ 0.52941176  0.58431373  0.60392157]\n",
      "   [ 0.79607843  0.84313725  0.8745098 ]]\n",
      "\n",
      "  [[ 0.27058824  0.28627451  0.2745098 ]\n",
      "   [ 0.32941176  0.34901961  0.38039216]\n",
      "   [ 0.26666667  0.29411765  0.31764706]\n",
      "   ..., \n",
      "   [ 0.33333333  0.37254902  0.34901961]\n",
      "   [ 0.27843137  0.32156863  0.31372549]\n",
      "   [ 0.47058824  0.52156863  0.52941176]]\n",
      "\n",
      "  [[ 0.27058824  0.28627451  0.2745098 ]\n",
      "   [ 0.35294118  0.37254902  0.39215686]\n",
      "   [ 0.24313725  0.27843137  0.29019608]\n",
      "   ..., \n",
      "   [ 0.29019608  0.31764706  0.2745098 ]\n",
      "   [ 0.20784314  0.24313725  0.21176471]\n",
      "   [ 0.24313725  0.29019608  0.27058824]]\n",
      "\n",
      "  ..., \n",
      "  [[ 0.48235294  0.50196078  0.37647059]\n",
      "   [ 0.51764706  0.51764706  0.4       ]\n",
      "   [ 0.50588235  0.50196078  0.39215686]\n",
      "   ..., \n",
      "   [ 0.42352941  0.41960784  0.34509804]\n",
      "   [ 0.24313725  0.23529412  0.21568627]\n",
      "   [ 0.10588235  0.10588235  0.10980392]]\n",
      "\n",
      "  [[ 0.45098039  0.4745098   0.35686275]\n",
      "   [ 0.48235294  0.48627451  0.37254902]\n",
      "   [ 0.50588235  0.49411765  0.38823529]\n",
      "   ..., \n",
      "   [ 0.45098039  0.45490196  0.36862745]\n",
      "   [ 0.25882353  0.25490196  0.23137255]\n",
      "   [ 0.10588235  0.10588235  0.10588235]]\n",
      "\n",
      "  [[ 0.45490196  0.47058824  0.35294118]\n",
      "   [ 0.4745098   0.47843137  0.36862745]\n",
      "   [ 0.50588235  0.50196078  0.39607843]\n",
      "   ..., \n",
      "   [ 0.45490196  0.45098039  0.36862745]\n",
      "   [ 0.26666667  0.25490196  0.22745098]\n",
      "   [ 0.10588235  0.10196078  0.10196078]]]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape=[None, image_shape[0], image_shape[1], image_shape[2]], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=[None, n_classes], name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=None, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "   # kernel and bias\n",
    "    filter_size_height = conv_ksize[0]\n",
    "    filter_size_width = conv_ksize[1]\n",
    "    x_shape = x_tensor.shape.as_list()\n",
    "    x_depth = x_shape[3]\n",
    "    \n",
    "    conv_init_values = tf.truncated_normal([filter_size_height, filter_size_width, x_depth, conv_num_outputs], stddev=0.1)    \n",
    "    kernel = tf.Variable(conv_init_values)\n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "\n",
    "    # Apply Convolution\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, kernel, strides=[1, conv_strides[0], conv_strides[1], 1], padding='SAME', data_format='NHWC')\n",
    "    # Add bias\n",
    "    conv_layer_bias = conv_layer + bias#tf.nn.bias_add(conv_layer, bias)\n",
    "    # Apply activation function\n",
    "    conv_layer_activated = tf.nn.relu(conv_layer_bias)\n",
    "    \n",
    "    # TODO: Implement Function\n",
    "     # Set the ksize (filter size) for each dimension (batch_size, height, width, depth)\n",
    "    ksize = [1, pool_ksize[0], pool_ksize[1], 1]\n",
    "    max_pool = tf.nn.max_pool(conv_layer_activated, ksize, strides=[1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "\n",
    "    return max_pool\n",
    "    \n",
    "    #conv = tf.layers.conv2d(x_tensor, conv_num_outputs, conv_ksize, strides=conv_strides, padding='SAME')\n",
    "    #maxp = tf.contrib.layers.max_pool2d(conv, pool_ksize, stride=pool_strides, padding='SAME')\n",
    "    #return maxp\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # shape is (?, 10, 30, 6), want to make it (?, 1800)\n",
    "    shape = x_tensor.shape.as_list()\n",
    "    reshaped = tf.reshape(x_tensor, [-1, shape[1] * shape[2] * shape[3]])\n",
    "    return reshaped\n",
    "    #tf.contrib.layers.flatten(x_tensor) #reshaped\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_shape = x_tensor.shape.as_list()\n",
    "    x_shape_b = x_shape[0]\n",
    "    x_shape_row = x_shape[1]\n",
    "  \n",
    "    weight = tf.Variable(tf.truncated_normal([x_shape_row, num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    out = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    activated = tf.nn.relu(out)\n",
    "    return activated\n",
    "#tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_shape = x_tensor.shape.as_list()\n",
    "    x_shape_b = x_shape[0]\n",
    "    x_shape_row = x_shape[1]\n",
    "    weight = tf.Variable(tf.truncated_normal([x_shape_row, num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    out = tf.add(tf.matmul(x_tensor, weight), bias)\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_num_outputs = 30\n",
    "    conv_ksize = (4,4)\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (2,2)\n",
    "    pool_strides = (2,2)\n",
    "    conv1 = conv2d_maxpool(x, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv_num_outputs2 = 50\n",
    "    conv_ksize2 = (2,2)\n",
    "    conv_strides2 = (2,2)\n",
    "    pool_ksize2 = (4,4)\n",
    "    pool_strides2 = (1,1)\n",
    "    conv2 = conv2d_maxpool(conv1, conv_num_outputs2, conv_ksize2, conv_strides2, pool_ksize2, pool_strides2)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    flattened_x = flatten(conv2)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    f_conn = fully_conn(flattened_x, 40)\n",
    "    f_conn_d = tf.nn.dropout(f_conn, keep_prob)\n",
    "    f_conn2 = fully_conn(f_conn_d, 40)\n",
    "    f_conn2_d = tf.nn.dropout(f_conn2, keep_prob)\n",
    "    f_conn3 = fully_conn(f_conn2_d, 40)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(f_conn3, 10)\n",
    "    #out = tf.nn.softmax(out)\n",
    "    # TODO: return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    labels = neural_net_label_input(10)\n",
    "    \n",
    "    # Run optimizer and get loss\n",
    "    session.run([optimizer, cost], feed_dict={x: feature_batch, y: label_batch, keep_prob:keep_probability})\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    global valid_features\n",
    "    global valid_labels\n",
    "    # TODO: Implement Function\n",
    "   \n",
    "    cost_dict = {x: feature_batch, y: label_batch, keep_prob: 1}\n",
    "    cost_value = session.run(cost, feed_dict=cost_dict)\n",
    "    \n",
    "    validation_feed_dict = {x: valid_features, y: valid_labels, keep_prob: 1}\n",
    "    validation_accuracy = session.run(accuracy, feed_dict=validation_feed_dict)\n",
    "\n",
    "    print('loss:{0}, validation accuracy:{1}'.format(cost_value, validation_accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 50\n",
    "batch_size = 1000\n",
    "keep_probability = 0.70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:2.2884764671325684, validation accuracy:0.16559998691082\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:2.226818084716797, validation accuracy:0.1785999983549118\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:2.120431661605835, validation accuracy:0.18599998950958252\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:2.037074327468872, validation accuracy:0.24639999866485596\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:1.9493303298950195, validation accuracy:0.27239999175071716\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:1.8918914794921875, validation accuracy:0.3043999671936035\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:1.8366172313690186, validation accuracy:0.3149999976158142\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:1.811669945716858, validation accuracy:0.32739999890327454\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:1.7732326984405518, validation accuracy:0.3391999900341034\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:1.7438443899154663, validation accuracy:0.3465999960899353\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:1.7242902517318726, validation accuracy:0.3537999987602234\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:1.715358018875122, validation accuracy:0.35839998722076416\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:1.6918106079101562, validation accuracy:0.36800000071525574\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:1.665757179260254, validation accuracy:0.3747999668121338\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:1.640437364578247, validation accuracy:0.3824000060558319\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:1.6165955066680908, validation accuracy:0.3901999890804291\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:1.603496789932251, validation accuracy:0.3903999924659729\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:1.5853335857391357, validation accuracy:0.3933999538421631\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:1.565110445022583, validation accuracy:0.4083999693393707\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:1.5632197856903076, validation accuracy:0.42139995098114014\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:1.5432195663452148, validation accuracy:0.420199990272522\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:1.506147861480713, validation accuracy:0.42319995164871216\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:1.4856157302856445, validation accuracy:0.43059998750686646\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:1.4673922061920166, validation accuracy:0.4333999752998352\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:1.455578327178955, validation accuracy:0.4429999589920044\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:1.4540951251983643, validation accuracy:0.4413999617099762\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:1.4199352264404297, validation accuracy:0.45339998602867126\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:1.4121792316436768, validation accuracy:0.4477999806404114\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:1.3799687623977661, validation accuracy:0.4487999677658081\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:1.3877843618392944, validation accuracy:0.44979995489120483\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:1.3764703273773193, validation accuracy:0.46279996633529663\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:1.3694512844085693, validation accuracy:0.47439998388290405\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:1.3538612127304077, validation accuracy:0.4681999385356903\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:1.336046576499939, validation accuracy:0.46119996905326843\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:1.3179490566253662, validation accuracy:0.47259998321533203\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:1.3145732879638672, validation accuracy:0.4769999384880066\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:1.2933688163757324, validation accuracy:0.48399996757507324\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:1.2829139232635498, validation accuracy:0.4885999858379364\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:1.2939659357070923, validation accuracy:0.4903999865055084\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:1.275308609008789, validation accuracy:0.48539993166923523\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:1.2543110847473145, validation accuracy:0.49140000343322754\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:1.2367478609085083, validation accuracy:0.49459999799728394\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:1.2474050521850586, validation accuracy:0.4987999200820923\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:1.2237602472305298, validation accuracy:0.4975999593734741\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:1.2302210330963135, validation accuracy:0.4915999472141266\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:1.2210479974746704, validation accuracy:0.5015999674797058\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:1.2261334657669067, validation accuracy:0.49959996342658997\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:1.194263219833374, validation accuracy:0.49859994649887085\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:1.1845636367797852, validation accuracy:0.5013999342918396\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:1.1719753742218018, validation accuracy:0.5067999362945557\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss:2.2956016063690186, validation accuracy:0.10740000009536743\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss:2.2845358848571777, validation accuracy:0.10859999060630798\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss:2.246739625930786, validation accuracy:0.20399998128414154\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss:2.1569745540618896, validation accuracy:0.20459997653961182\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss:2.10636305809021, validation accuracy:0.2345999926328659\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss:2.0435972213745117, validation accuracy:0.2563999891281128\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss:1.9766134023666382, validation accuracy:0.2913999557495117\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss:1.9177587032318115, validation accuracy:0.3139999806880951\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss:1.855571985244751, validation accuracy:0.3214000165462494\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss:1.8390499353408813, validation accuracy:0.3188000023365021\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss:1.8066340684890747, validation accuracy:0.3409999907016754\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss:1.795675277709961, validation accuracy:0.3294000029563904\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss:1.731979489326477, validation accuracy:0.35199999809265137\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss:1.7244281768798828, validation accuracy:0.3619999885559082\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss:1.725764513015747, validation accuracy:0.353799968957901\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss:1.6968355178833008, validation accuracy:0.3709999918937683\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss:1.71579110622406, validation accuracy:0.38079994916915894\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss:1.6350206136703491, validation accuracy:0.3781999945640564\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss:1.6403634548187256, validation accuracy:0.38279998302459717\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss:1.652453899383545, validation accuracy:0.3869999647140503\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss:1.6045103073120117, validation accuracy:0.4099999666213989\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss:1.6419934034347534, validation accuracy:0.4032000005245209\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss:1.5647287368774414, validation accuracy:0.41040000319480896\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss:1.56325101852417, validation accuracy:0.41200000047683716\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss:1.597515344619751, validation accuracy:0.4033999443054199\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss:1.5534610748291016, validation accuracy:0.4283999502658844\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss:1.5950706005096436, validation accuracy:0.43299996852874756\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss:1.5058420896530151, validation accuracy:0.4307999610900879\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss:1.5033011436462402, validation accuracy:0.42659997940063477\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss:1.5465773344039917, validation accuracy:0.4375999867916107\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss:1.493417739868164, validation accuracy:0.44599997997283936\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss:1.5571393966674805, validation accuracy:0.4472000002861023\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss:1.4642984867095947, validation accuracy:0.4357999563217163\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss:1.4718447923660278, validation accuracy:0.4485999643802643\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss:1.5051952600479126, validation accuracy:0.45319995284080505\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss:1.4658797979354858, validation accuracy:0.45799997448921204\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss:1.5304359197616577, validation accuracy:0.4511999785900116\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss:1.460968255996704, validation accuracy:0.44419997930526733\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss:1.4438791275024414, validation accuracy:0.4617999792098999\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss:1.4679170846939087, validation accuracy:0.4607999920845032\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss:1.4514012336730957, validation accuracy:0.4529999792575836\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss:1.4936864376068115, validation accuracy:0.4745999574661255\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss:1.4185878038406372, validation accuracy:0.46219998598098755\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss:1.4247297048568726, validation accuracy:0.4691999554634094\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss:1.457332730293274, validation accuracy:0.465999960899353\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss:1.4309847354888916, validation accuracy:0.47039997577667236\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss:1.4843443632125854, validation accuracy:0.47499996423721313\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss:1.4234598875045776, validation accuracy:0.4615999162197113\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss:1.409998893737793, validation accuracy:0.478799968957901\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss:1.4347330331802368, validation accuracy:0.4721999764442444\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss:1.397831916809082, validation accuracy:0.48719990253448486\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss:1.4450329542160034, validation accuracy:0.48639994859695435\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss:1.3862721920013428, validation accuracy:0.47259992361068726\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss:1.3789664506912231, validation accuracy:0.4925999939441681\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss:1.4135539531707764, validation accuracy:0.480199933052063\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss:1.3810083866119385, validation accuracy:0.48879992961883545\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss:1.4184318780899048, validation accuracy:0.49859997630119324\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss:1.3776248693466187, validation accuracy:0.47519999742507935\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss:1.3490930795669556, validation accuracy:0.4967999756336212\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss:1.408036708831787, validation accuracy:0.4917999505996704\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss:1.37723708152771, validation accuracy:0.49539998173713684\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss:1.4119805097579956, validation accuracy:0.500999927520752\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss:1.3338632583618164, validation accuracy:0.4925999641418457\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss:1.364219307899475, validation accuracy:0.49699991941452026\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss:1.3661679029464722, validation accuracy:0.5069999694824219\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss:1.35634183883667, validation accuracy:0.5059999227523804\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss:1.3980746269226074, validation accuracy:0.49939993023872375\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss:1.3240503072738647, validation accuracy:0.4991999566555023\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss:1.3301570415496826, validation accuracy:0.506399929523468\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss:1.3523945808410645, validation accuracy:0.5083999633789062\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss:1.3360875844955444, validation accuracy:0.5123999714851379\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss:1.3584297895431519, validation accuracy:0.5197999477386475\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss:1.3184000253677368, validation accuracy:0.511199951171875\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss:1.2964316606521606, validation accuracy:0.5181999802589417\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss:1.3242945671081543, validation accuracy:0.5191999673843384\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss:1.3226652145385742, validation accuracy:0.5105999708175659\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss:1.3464000225067139, validation accuracy:0.5205999612808228\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss:1.281388759613037, validation accuracy:0.5207999348640442\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss:1.2648890018463135, validation accuracy:0.5269999504089355\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss:1.3191709518432617, validation accuracy:0.5169999599456787\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss:1.3006662130355835, validation accuracy:0.5257999300956726\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss:1.334829568862915, validation accuracy:0.5315999388694763\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss:1.2632678747177124, validation accuracy:0.5213999152183533\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss:1.2760698795318604, validation accuracy:0.5231999754905701\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss:1.2912808656692505, validation accuracy:0.514799952507019\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss:1.2879124879837036, validation accuracy:0.5357999801635742\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss:1.325295090675354, validation accuracy:0.5437999367713928\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss:1.2911086082458496, validation accuracy:0.511199951171875\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss:1.2621822357177734, validation accuracy:0.5361999273300171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, CIFAR-10 Batch 5:  loss:1.3037266731262207, validation accuracy:0.5225999355316162\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss:1.2829283475875854, validation accuracy:0.535599946975708\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss:1.3064028024673462, validation accuracy:0.533799946308136\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss:1.2803770303726196, validation accuracy:0.5185999870300293\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss:1.2627149820327759, validation accuracy:0.5399999618530273\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss:1.261196255683899, validation accuracy:0.5365999341011047\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss:1.2477245330810547, validation accuracy:0.5445999503135681\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss:1.281449556350708, validation accuracy:0.5445999503135681\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss:1.2180181741714478, validation accuracy:0.544999897480011\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss:1.2186487913131714, validation accuracy:0.5467999577522278\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss:1.2425203323364258, validation accuracy:0.5407999157905579\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss:1.235855221748352, validation accuracy:0.5493999123573303\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss:1.274004578590393, validation accuracy:0.5595999360084534\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss:1.2208149433135986, validation accuracy:0.5425999164581299\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss:1.2346445322036743, validation accuracy:0.5399999022483826\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss:1.2415075302124023, validation accuracy:0.5393999814987183\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss:1.2492895126342773, validation accuracy:0.5469999313354492\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss:1.2715632915496826, validation accuracy:0.5441999435424805\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss:1.204849362373352, validation accuracy:0.5505999326705933\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss:1.1825488805770874, validation accuracy:0.5546000003814697\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss:1.2179657220840454, validation accuracy:0.55159991979599\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss:1.2198580503463745, validation accuracy:0.5489999651908875\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss:1.2278649806976318, validation accuracy:0.5477999448776245\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss:1.2073382139205933, validation accuracy:0.5465999841690063\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss:1.2015113830566406, validation accuracy:0.5503999590873718\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss:1.2082390785217285, validation accuracy:0.5491999387741089\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss:1.2223763465881348, validation accuracy:0.5449999570846558\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss:1.231830358505249, validation accuracy:0.5519999265670776\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss:1.1771079301834106, validation accuracy:0.5583999156951904\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss:1.1818867921829224, validation accuracy:0.5633999705314636\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss:1.2085399627685547, validation accuracy:0.5489999055862427\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss:1.2012537717819214, validation accuracy:0.5531998872756958\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss:1.2305908203125, validation accuracy:0.5523999333381653\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss:1.1856184005737305, validation accuracy:0.556999921798706\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss:1.175447940826416, validation accuracy:0.5623998641967773\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss:1.2012919187545776, validation accuracy:0.5503999590873718\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss:1.1815319061279297, validation accuracy:0.5623999238014221\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss:1.2087392807006836, validation accuracy:0.5565999746322632\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss:1.179335355758667, validation accuracy:0.5635999441146851\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss:1.1625633239746094, validation accuracy:0.5607999563217163\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss:1.1954381465911865, validation accuracy:0.549799919128418\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss:1.1906023025512695, validation accuracy:0.5623999238014221\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss:1.2046374082565308, validation accuracy:0.5629999041557312\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss:1.1713954210281372, validation accuracy:0.5521999597549438\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss:1.161352515220642, validation accuracy:0.5589999556541443\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss:1.1911423206329346, validation accuracy:0.5557999610900879\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss:1.161761999130249, validation accuracy:0.5669999122619629\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss:1.2040187120437622, validation accuracy:0.5657999515533447\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss:1.1750770807266235, validation accuracy:0.5655999183654785\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss:1.1576330661773682, validation accuracy:0.5625999569892883\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss:1.1781573295593262, validation accuracy:0.5615999102592468\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss:1.1708415746688843, validation accuracy:0.5669999122619629\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss:1.1873340606689453, validation accuracy:0.5639998912811279\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss:1.1628153324127197, validation accuracy:0.5685999393463135\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss:1.1495100259780884, validation accuracy:0.5649999380111694\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss:1.1753039360046387, validation accuracy:0.5601999163627625\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss:1.1536322832107544, validation accuracy:0.5727999806404114\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss:1.1853652000427246, validation accuracy:0.5719999670982361\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss:1.1543787717819214, validation accuracy:0.5697999596595764\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss:1.1343382596969604, validation accuracy:0.5671999454498291\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss:1.143889307975769, validation accuracy:0.5601999163627625\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss:1.144479751586914, validation accuracy:0.5695999264717102\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss:1.1780917644500732, validation accuracy:0.5671998858451843\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss:1.1457345485687256, validation accuracy:0.5637999773025513\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss:1.1227333545684814, validation accuracy:0.5737999081611633\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss:1.1794145107269287, validation accuracy:0.5535999536514282\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss:1.1556228399276733, validation accuracy:0.5749998688697815\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss:1.1842361688613892, validation accuracy:0.5667999386787415\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss:1.143429160118103, validation accuracy:0.5740000009536743\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss:1.1329858303070068, validation accuracy:0.5699999332427979\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss:1.1546952724456787, validation accuracy:0.5623999238014221\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss:1.1506907939910889, validation accuracy:0.572399914264679\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss:1.1648457050323486, validation accuracy:0.5745999217033386\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss:1.1307200193405151, validation accuracy:0.579599916934967\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss:1.1284916400909424, validation accuracy:0.5707999467849731\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss:1.125882625579834, validation accuracy:0.5677999258041382\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss:1.127902865409851, validation accuracy:0.5763999223709106\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss:1.1605451107025146, validation accuracy:0.574999988079071\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss:1.1244819164276123, validation accuracy:0.5805999040603638\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss:1.1139881610870361, validation accuracy:0.5747999548912048\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss:1.137041687965393, validation accuracy:0.5723999738693237\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss:1.1479637622833252, validation accuracy:0.5785999298095703\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss:1.1406937837600708, validation accuracy:0.5743999481201172\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss:1.1240757703781128, validation accuracy:0.5747999548912048\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss:1.11076819896698, validation accuracy:0.5693999528884888\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss:1.1360419988632202, validation accuracy:0.5671999454498291\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss:1.1515064239501953, validation accuracy:0.5733999013900757\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss:1.1538727283477783, validation accuracy:0.5705999135971069\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss:1.089514136314392, validation accuracy:0.5799999237060547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, CIFAR-10 Batch 4:  loss:1.1039116382598877, validation accuracy:0.5845999717712402\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss:1.1191554069519043, validation accuracy:0.5699999332427979\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss:1.113561987876892, validation accuracy:0.579599916934967\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss:1.1447434425354004, validation accuracy:0.5763999223709106\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss:1.1070334911346436, validation accuracy:0.5759998559951782\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss:1.093078374862671, validation accuracy:0.585399866104126\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss:1.1241759061813354, validation accuracy:0.5767999887466431\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss:1.10075843334198, validation accuracy:0.5797999501228333\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss:1.122943639755249, validation accuracy:0.5779998898506165\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss:1.0899995565414429, validation accuracy:0.5849999189376831\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss:1.084702730178833, validation accuracy:0.5775998830795288\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss:1.1239137649536133, validation accuracy:0.5749999284744263\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss:1.1157832145690918, validation accuracy:0.5771998763084412\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss:1.1234365701675415, validation accuracy:0.5821999311447144\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss:1.0835151672363281, validation accuracy:0.5887999534606934\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss:1.0793886184692383, validation accuracy:0.5867998600006104\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss:1.10209059715271, validation accuracy:0.5831998586654663\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss:1.1135129928588867, validation accuracy:0.5873998999595642\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss:1.1145544052124023, validation accuracy:0.582599937915802\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss:1.0788288116455078, validation accuracy:0.5865998864173889\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss:1.0967223644256592, validation accuracy:0.5825998783111572\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss:1.1026601791381836, validation accuracy:0.5771999359130859\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss:1.1024975776672363, validation accuracy:0.5833999514579773\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss:1.1039369106292725, validation accuracy:0.5851999521255493\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss:1.0839529037475586, validation accuracy:0.5857998728752136\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss:1.0895183086395264, validation accuracy:0.5835999250411987\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss:1.0907175540924072, validation accuracy:0.5867999196052551\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss:1.0913841724395752, validation accuracy:0.5871999263763428\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss:1.1040421724319458, validation accuracy:0.5857999324798584\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss:1.076137661933899, validation accuracy:0.5887998938560486\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss:1.0875484943389893, validation accuracy:0.5791999101638794\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss:1.0916831493377686, validation accuracy:0.584399938583374\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss:1.0880839824676514, validation accuracy:0.5903999209403992\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss:1.095980167388916, validation accuracy:0.5831999778747559\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss:1.0841095447540283, validation accuracy:0.5859999060630798\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss:1.06294846534729, validation accuracy:0.5881999135017395\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss:1.0781551599502563, validation accuracy:0.5963998436927795\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss:1.0898476839065552, validation accuracy:0.5897999405860901\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss:1.1034032106399536, validation accuracy:0.578999936580658\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss:1.0755497217178345, validation accuracy:0.5965999364852905\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss:1.0554643869400024, validation accuracy:0.5863999128341675\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss:1.0726767778396606, validation accuracy:0.5895999073982239\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss:1.0807831287384033, validation accuracy:0.5925998687744141\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss:1.1033802032470703, validation accuracy:0.5849999785423279\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss:1.051418662071228, validation accuracy:0.5963999032974243\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss:1.0594817399978638, validation accuracy:0.5879999399185181\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss:1.084542155265808, validation accuracy:0.5849999189376831\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss:1.0892736911773682, validation accuracy:0.5877999067306519\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss:1.096194863319397, validation accuracy:0.5889999270439148\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss:1.0616264343261719, validation accuracy:0.5891999006271362\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss:1.0702688694000244, validation accuracy:0.5887998938560486\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss:1.0770666599273682, validation accuracy:0.5849999189376831\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss:1.0724297761917114, validation accuracy:0.5897998809814453\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss:1.0843201875686646, validation accuracy:0.5871999263763428\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss:1.0535601377487183, validation accuracy:0.5963999032974243\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss:1.0536279678344727, validation accuracy:0.5891999006271362\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss:1.0679081678390503, validation accuracy:0.5933998823165894\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss:1.0838167667388916, validation accuracy:0.5895998477935791\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss:1.088482141494751, validation accuracy:0.5857999324798584\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss:1.0489251613616943, validation accuracy:0.5931999087333679\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss:1.035463571548462, validation accuracy:0.5911999344825745\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss:1.070777416229248, validation accuracy:0.5885999202728271\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss:1.0548814535140991, validation accuracy:0.5883999466896057\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss:1.0655596256256104, validation accuracy:0.5971999168395996\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss:1.0388429164886475, validation accuracy:0.595599889755249\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss:1.0467922687530518, validation accuracy:0.597399890422821\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss:1.062237024307251, validation accuracy:0.5875998735427856\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss:1.0476188659667969, validation accuracy:0.6013998985290527\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss:1.0676943063735962, validation accuracy:0.5933998823165894\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss:1.033489465713501, validation accuracy:0.5993998646736145\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss:1.027417778968811, validation accuracy:0.5941999554634094\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss:1.04640531539917, validation accuracy:0.6017999649047852\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6059999883174896\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8381937ef0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
